{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"execution":{"iopub.status.busy":"2021-11-28T04:37:25.070059Z","iopub.execute_input":"2021-11-28T04:37:25.070368Z","iopub.status.idle":"2021-11-28T04:37:26.222036Z","shell.execute_reply.started":"2021-11-28T04:37:25.070287Z","shell.execute_reply":"2021-11-28T04:37:26.221187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# General imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import regularizers\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.models import load_model\nimport seaborn as sns\nfrom mlxtend.plotting import plot_decision_regions\nfrom sklearn.metrics import confusion_matrix\nimport cv2\nimport PIL","metadata":{"execution":{"iopub.status.busy":"2021-11-28T04:37:29.199985Z","iopub.execute_input":"2021-11-28T04:37:29.200241Z","iopub.status.idle":"2021-11-28T04:37:34.920737Z","shell.execute_reply.started":"2021-11-28T04:37:29.200211Z","shell.execute_reply":"2021-11-28T04:37:34.919964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set up ImageDataGenerator\ntrain_imagegen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255,\n                                   zoom_range=[0.6,1],\n                                   rotation_range=10,\n                                   brightness_range=([0.6, 1.5]),\n                                   horizontal_flip=True,\n                                   validation_split=0.06) # this will set aside a part of training set for validation data\ntest_imagegen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255,\n                                   zoom_range=[0.6,1],\n                                   rotation_range=10,\n                                   brightness_range=([0.6, 1.5]),\n                                   horizontal_flip=True)\n# Bring the data in\ntrain_generator = train_imagegen.flow_from_directory(\n                                    '../input/brain-tumor-classification-mri/Training',\n                                    target_size=(200,200),\n                                    batch_size=20,\n                                    class_mode='categorical',\n                                    subset='training')\n\ntest_generator = test_imagegen.flow_from_directory(\n                                    '../input/brain-tumor-classification-mri/Testing',\n                                    target_size=(200,200),\n                                    batch_size=20,\n                                    class_mode='categorical')\n\nval_generator = train_imagegen.flow_from_directory(\n                                    '../input/brain-tumor-classification-mri/Training',\n                                    target_size=(200,200),\n                                    batch_size=20,\n                                    class_mode='categorical',\n                                    subset='validation')","metadata":{"execution":{"iopub.status.busy":"2021-11-28T04:37:39.818846Z","iopub.execute_input":"2021-11-28T04:37:39.819614Z","iopub.status.idle":"2021-11-28T04:37:40.244889Z","shell.execute_reply.started":"2021-11-28T04:37:39.819575Z","shell.execute_reply":"2021-11-28T04:37:40.243486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_training_results(history):\n    '''\n    From https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n    \n    Input: keras history object (output from trained model)\n    '''\n    fig, (ax1, ax2) = plt.subplots(2, sharex=True)\n    fig.suptitle('Model Results')\n\n    # summarize history for accuracy\n    ax1.plot(history.history['acc'])\n    ax1.plot(history.history['val_acc'])\n    ax1.set_ylabel('Accuracy')\n    ax1.legend(['train', 'test'], loc='upper left')\n    # summarize history for loss\n    ax2.plot(history.history['loss'])\n    ax2.plot(history.history['val_loss'])\n    ax2.set_ylabel('Loss')\n    ax2.legend(['train', 'test'], loc='upper left')\n    \n    plt.xlabel('Epoch')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-28T04:37:45.163277Z","iopub.execute_input":"2021-11-28T04:37:45.164018Z","iopub.status.idle":"2021-11-28T04:37:45.170768Z","shell.execute_reply.started":"2021-11-28T04:37:45.163974Z","shell.execute_reply":"2021-11-28T04:37:45.169998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize (code from https://github.com/austint1121/OES-PneumoniaClassification/blob/main/Final_Notebook.ipynb)\ntrain_batch = train_generator.next()\nfig, axes = plt.subplots(2, 5, figsize=(16, 8))\n    \nfor i in range(10):\n    # Load image into numpy array and re-scale\n    img = np.array(train_batch[0][i] * 255, dtype='uint8')\n    ax = axes[i // 5, i % 5]\n    ax.imshow(img)\nfig.suptitle('Training Images')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-25T20:03:13.087432Z","iopub.execute_input":"2021-11-25T20:03:13.087687Z","iopub.status.idle":"2021-11-25T20:03:14.842578Z","shell.execute_reply.started":"2021-11-25T20:03:13.087658Z","shell.execute_reply":"2021-11-25T20:03:14.841925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Getting a look at the Distribution of Different Tumor Types**","metadata":{}},{"cell_type":"code","source":"train_generator.mode","metadata":{"execution":{"iopub.status.busy":"2021-11-25T20:04:59.126852Z","iopub.execute_input":"2021-11-25T20:04:59.127133Z","iopub.status.idle":"2021-11-25T20:04:59.409719Z","shell.execute_reply.started":"2021-11-25T20:04:59.127102Z","shell.execute_reply":"2021-11-25T20:04:59.408769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_generator.class_indices","metadata":{"execution":{"iopub.status.busy":"2021-11-28T00:22:28.362864Z","iopub.execute_input":"2021-11-28T00:22:28.363142Z","iopub.status.idle":"2021-11-28T00:22:28.370832Z","shell.execute_reply.started":"2021-11-28T00:22:28.363113Z","shell.execute_reply":"2021-11-28T00:22:28.370068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_generator[0].","metadata":{"execution":{"iopub.status.busy":"2021-11-28T00:23:14.311451Z","iopub.execute_input":"2021-11-28T00:23:14.311712Z","iopub.status.idle":"2021-11-28T00:23:14.559205Z","shell.execute_reply.started":"2021-11-28T00:23:14.311684Z","shell.execute_reply":"2021-11-28T00:23:14.558307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tumor_types = pd.DataFrame(train_generator.classes)\ntrain_values = train_tumor_types.value_counts()\ntrain_values","metadata":{"execution":{"iopub.status.busy":"2021-11-25T20:08:25.372925Z","iopub.execute_input":"2021-11-25T20:08:25.373626Z","iopub.status.idle":"2021-11-25T20:08:25.395737Z","shell.execute_reply.started":"2021-11-25T20:08:25.373591Z","shell.execute_reply":"2021-11-25T20:08:25.394976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tumor_types.rename(columns={0:'Tumor Type'}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T20:08:57.108119Z","iopub.execute_input":"2021-11-25T20:08:57.108384Z","iopub.status.idle":"2021-11-25T20:08:57.114168Z","shell.execute_reply.started":"2021-11-25T20:08:57.108354Z","shell.execute_reply":"2021-11-25T20:08:57.113496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preparations for visualization\nno_tumor = len(train_tumor_types[train_tumor_types['Tumor Type'] == 2])\nglioma = len(train_tumor_types[train_tumor_types['Tumor Type'] == 0])\nmeningioma = len(train_tumor_types[train_tumor_types['Tumor Type'] == 1])\npituitary = len(train_tumor_types[train_tumor_types['Tumor Type'] == 3])","metadata":{"execution":{"iopub.status.busy":"2021-11-25T20:13:46.171772Z","iopub.execute_input":"2021-11-25T20:13:46.172134Z","iopub.status.idle":"2021-11-25T20:13:46.182735Z","shell.execute_reply.started":"2021-11-25T20:13:46.172098Z","shell.execute_reply":"2021-11-25T20:13:46.181544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfig, ax = plt.subplots(figsize=(10,8))\nax.bar(x=['No Tumor', 'Glioma', 'Meningioma', 'Pituitary'], height = [no_tumor, glioma, meningioma, pituitary])\nax.set(xlabel='', ylabel='Number of Images', title='Distribution of Brain Tumor Type');\n","metadata":{"execution":{"iopub.status.busy":"2021-11-25T20:15:41.499701Z","iopub.execute_input":"2021-11-25T20:15:41.499958Z","iopub.status.idle":"2021-11-25T20:15:41.728519Z","shell.execute_reply.started":"2021-11-25T20:15:41.49993Z","shell.execute_reply":"2021-11-25T20:15:41.727852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Taking a Look at Different Tumor Types**","metadata":{}},{"cell_type":"code","source":"# View a Glioma tumor\nglioma1 = PIL.Image.open('../input/brain-tumor-classification-mri/Training/glioma_tumor/gg (1).jpg')\nglioma1","metadata":{"execution":{"iopub.status.busy":"2021-11-25T20:05:31.132865Z","iopub.execute_input":"2021-11-25T20:05:31.133548Z","iopub.status.idle":"2021-11-25T20:05:31.193916Z","shell.execute_reply.started":"2021-11-25T20:05:31.133505Z","shell.execute_reply":"2021-11-25T20:05:31.19324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"glioma1.mode","metadata":{"execution":{"iopub.status.busy":"2021-11-25T20:05:37.26158Z","iopub.execute_input":"2021-11-25T20:05:37.262149Z","iopub.status.idle":"2021-11-25T20:05:37.266961Z","shell.execute_reply.started":"2021-11-25T20:05:37.26211Z","shell.execute_reply":"2021-11-25T20:05:37.2663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"glioma1.size","metadata":{"execution":{"iopub.status.busy":"2021-11-21T17:13:51.082579Z","iopub.execute_input":"2021-11-21T17:13:51.082895Z","iopub.status.idle":"2021-11-21T17:13:51.089191Z","shell.execute_reply.started":"2021-11-21T17:13:51.082864Z","shell.execute_reply":"2021-11-21T17:13:51.088536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **First Baseline Model**","metadata":{}},{"cell_type":"code","source":"baseline = keras.Sequential()\nbaseline.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(200,200,3)))\nbaseline.add(layers.MaxPooling2D(2,2))\nbaseline.add(layers.Conv2D(64, (3,3), activation='relu'))\nbaseline.add(layers.MaxPooling2D(2,2))\n\nbaseline.add(layers.Flatten())\nbaseline.add(layers.Dense(128, activation='relu'))\nbaseline.add(layers.Dense(4, activation='softmax'))\n\nbaseline.compile(loss='categorical_crossentropy',\n                optimizer='adam',\n                metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\n\nbaseline_results = baseline.fit_generator(train_generator,\n                                         steps_per_epoch=2699/20,# number of samples / batch size\n                                         epochs=10,\n                                         validation_data=test_generator)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T20:22:25.953994Z","iopub.execute_input":"2021-11-25T20:22:25.954878Z","iopub.status.idle":"2021-11-25T20:29:11.119966Z","shell.execute_reply.started":"2021-11-25T20:22:25.954839Z","shell.execute_reply":"2021-11-25T20:29:11.119094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_training_results(baseline_results)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T20:36:09.033761Z","iopub.execute_input":"2021-11-25T20:36:09.034034Z","iopub.status.idle":"2021-11-25T20:36:09.564634Z","shell.execute_reply.started":"2021-11-25T20:36:09.033984Z","shell.execute_reply":"2021-11-25T20:36:09.563951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Adding Class Weights to Baseline CNN**","metadata":{}},{"cell_type":"code","source":"base_weights = keras.Sequential()\nbase_weights.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(200,200,3)))\nbase_weights.add(layers.MaxPooling2D(2,2))\nbase_weights.add(layers.Conv2D(64, (3,3), activation='relu'))\nbase_weights.add(layers.MaxPooling2D(2,2))\n\nbase_weights.add(layers.Flatten())\nbase_weights.add(layers.Dense(128, activation='relu'))\nbase_weights.add(layers.Dense(4, activation='softmax'))\n\nbase_weights.compile(loss='categorical_crossentropy',\n                optimizer='adam',\n                metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\n\nmulti_weights = {0: 1,\n                1: 1 ,\n                2: 2,\n                3: 1,}\n\nbase_weights_results = base_weights.fit_generator(train_generator,\n                                          class_weight=multi_weights,\n                                         steps_per_epoch=2699/20,# number of samples / batch size\n                                         epochs=10,\n                                         validation_data=test_generator)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-26T03:46:38.979184Z","iopub.execute_input":"2021-11-26T03:46:38.979821Z","iopub.status.idle":"2021-11-26T03:53:37.606838Z","shell.execute_reply.started":"2021-11-26T03:46:38.979783Z","shell.execute_reply":"2021-11-26T03:53:37.606146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Adding Batch Normalization**","metadata":{}},{"cell_type":"code","source":"norm = keras.Sequential()\nnorm.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(200,200,3)))\nnorm.add(layers.BatchNormalization())\nnorm.add(layers.MaxPooling2D(2,2))\nnorm.add(layers.Conv2D(64, (3,3), activation='relu'))\nnorm.add(layers.BatchNormalization())\nnorm.add(layers.MaxPooling2D(2,2))\n\nnorm.add(layers.Flatten())\nnorm.add(layers.Dense(128, activation='relu'))\nnorm.add(layers.BatchNormalization())\nnorm.add(layers.Dense(4, activation='softmax'))\n\nnorm.compile(loss='categorical_crossentropy',\n                optimizer='adam',\n                metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\n\nmulti_weights = {0: 1,\n                1: 1 ,\n                2: 2,\n                3: 1,}\n\nnorm_results = norm.fit_generator(train_generator,\n                                          class_weight=multi_weights,\n                                         steps_per_epoch=2699/20,# number of samples / batch size\n                                         epochs=10,\n                                         validation_data=test_generator)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T03:54:13.138444Z","iopub.execute_input":"2021-11-26T03:54:13.138701Z","iopub.status.idle":"2021-11-26T04:00:58.782804Z","shell.execute_reply.started":"2021-11-26T03:54:13.138672Z","shell.execute_reply":"2021-11-26T04:00:58.781981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_training_results(norm_results)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T04:01:35.185314Z","iopub.execute_input":"2021-11-26T04:01:35.186134Z","iopub.status.idle":"2021-11-26T04:01:35.458833Z","shell.execute_reply.started":"2021-11-26T04:01:35.186086Z","shell.execute_reply":"2021-11-26T04:01:35.458167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Using Pre-Trained VGG-19 Weights**","metadata":{}},{"cell_type":"code","source":"from keras.applications.vgg19 import VGG19\ncnn_vgg = VGG19(weights='imagenet',\n               include_top=False,\n               input_shape=(200,200,3))","metadata":{"execution":{"iopub.status.busy":"2021-11-28T01:55:37.805785Z","iopub.execute_input":"2021-11-28T01:55:37.80608Z","iopub.status.idle":"2021-11-28T01:55:38.752215Z","shell.execute_reply.started":"2021-11-28T01:55:37.806047Z","shell.execute_reply":"2021-11-28T01:55:38.751174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making early stop for model\nearly_stop2 = [EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True),\n            ModelCheckpoint(filepath='best_model.h5', monitor='val_loss',\n                           save_best_only=True)]","metadata":{"execution":{"iopub.status.busy":"2021-11-28T04:38:06.807634Z","iopub.execute_input":"2021-11-28T04:38:06.808192Z","iopub.status.idle":"2021-11-28T04:38:06.812735Z","shell.execute_reply.started":"2021-11-28T04:38:06.808153Z","shell.execute_reply":"2021-11-28T04:38:06.811854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn_vgg.summary()","metadata":{"execution":{"iopub.status.busy":"2021-11-28T01:55:41.839179Z","iopub.execute_input":"2021-11-28T01:55:41.839667Z","iopub.status.idle":"2021-11-28T01:55:41.855475Z","shell.execute_reply.started":"2021-11-28T01:55:41.839633Z","shell.execute_reply":"2021-11-28T01:55:41.854767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build first model using pretrained VGG 19 as first layer, and then some dense layers on top\npretrained = keras.Sequential()\npretrained.add(cnn_vgg)\npretrained.add(layers.Flatten())\npretrained.add(layers.Dense(128, activation='relu'))\npretrained.add(layers.Dense(4, activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2021-11-28T00:27:43.774673Z","iopub.execute_input":"2021-11-28T00:27:43.775362Z","iopub.status.idle":"2021-11-28T00:27:43.853665Z","shell.execute_reply.started":"2021-11-28T00:27:43.775313Z","shell.execute_reply":"2021-11-28T00:27:43.852892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make the pretrianed layer untrainable so that during optimization, its weights don't change\ncnn_vgg.trainable = False","metadata":{"execution":{"iopub.status.busy":"2021-11-28T00:27:36.055131Z","iopub.execute_input":"2021-11-28T00:27:36.055665Z","iopub.status.idle":"2021-11-28T00:27:36.060241Z","shell.execute_reply.started":"2021-11-28T00:27:36.055628Z","shell.execute_reply":"2021-11-28T00:27:36.059431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check to see that the pretrained layer is not trainable but that all others are\nfor layer in pretrained.layers:\n    print(layer.name, layer.trainable)\n    \nprint(len(pretrained.trainable_weights))","metadata":{"execution":{"iopub.status.busy":"2021-11-28T00:27:46.390675Z","iopub.execute_input":"2021-11-28T00:27:46.391214Z","iopub.status.idle":"2021-11-28T00:27:46.398356Z","shell.execute_reply.started":"2021-11-28T00:27:46.391176Z","shell.execute_reply":"2021-11-28T00:27:46.397609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\npretrained.compile(loss='categorical_crossentropy',\n                optimizer='adam',\n                metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\n\npretrained_results = pretrained.fit_generator(train_generator,\n                                         steps_per_epoch=2699/20,# number of samples / batch size\n                                         epochs=20,\n                                        callbacks= early_stop2,\n                                         validation_data=test_generator)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T20:42:37.385713Z","iopub.execute_input":"2021-11-27T20:42:37.385972Z","iopub.status.idle":"2021-11-27T20:51:58.828156Z","shell.execute_reply.started":"2021-11-27T20:42:37.385943Z","shell.execute_reply":"2021-11-27T20:51:58.827407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_training_results(pretrained_results)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T03:06:35.765326Z","iopub.execute_input":"2021-11-27T03:06:35.76608Z","iopub.status.idle":"2021-11-27T03:06:36.075494Z","shell.execute_reply.started":"2021-11-27T03:06:35.766029Z","shell.execute_reply":"2021-11-27T03:06:36.074807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Analysis of Model**\n\nTraining accuracy is 97% and testing accuracy is 77%, while training loss is 9% and testing loss is 184%. Testing recall is 77%. The model is definitely overfit, there is a lot of loss, and it is important for recall to be higher, so this model requires a lot of tuning. In the next iteration I will see if unfreezing some of the outer layers so that they can train on these specific images will help imrpove the model.","metadata":{}},{"cell_type":"markdown","source":"## **Fine Tuning the Outer Layers of the Pretrained Network**","metadata":{}},{"cell_type":"code","source":"# Build first model using pretrained VGG 19 as first layer, and then some dense layers on top\nb5_c1 = keras.Sequential()\nb5_c1.add(cnn_vgg)\nb5_c1.add(layers.Flatten())\nb5_c1.add(layers.Dense(128, activation='relu'))\nb5_c1.add(layers.Dense(4, activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2021-11-27T20:52:40.131815Z","iopub.execute_input":"2021-11-27T20:52:40.132098Z","iopub.status.idle":"2021-11-27T20:52:40.42334Z","shell.execute_reply.started":"2021-11-27T20:52:40.13206Z","shell.execute_reply":"2021-11-27T20:52:40.422623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Unfreezing the base\ncnn_vgg.trainable = True","metadata":{"execution":{"iopub.status.busy":"2021-11-27T20:52:43.341547Z","iopub.execute_input":"2021-11-27T20:52:43.341805Z","iopub.status.idle":"2021-11-27T20:52:43.346342Z","shell.execute_reply.started":"2021-11-27T20:52:43.341775Z","shell.execute_reply":"2021-11-27T20:52:43.345459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Re-freezing everything except for the last layer of the pretrained CNN\n# Code structure from https://github.com/learn-co-curriculum/dsc-using-pretrained-networks-codealong\ncnn_vgg.trainable = True\nfor layer in  cnn_vgg.layers:\n    if layer.name == 'block5_conv1':\n        layer.trainable = True\n    else:\n        layer.trainable = False\n        ","metadata":{"execution":{"iopub.status.busy":"2021-11-28T00:28:01.504234Z","iopub.execute_input":"2021-11-28T00:28:01.50482Z","iopub.status.idle":"2021-11-28T00:28:01.511504Z","shell.execute_reply.started":"2021-11-28T00:28:01.504783Z","shell.execute_reply":"2021-11-28T00:28:01.510714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking to see that only the 'block5_conv1' layer is unfrozen\nfor layer in cnn_vgg.layers:\n    print(layer.name, layer.trainable)\n    \nprint(len(cnn_vgg.trainable_weights))","metadata":{"execution":{"iopub.status.busy":"2021-11-28T00:28:03.314519Z","iopub.execute_input":"2021-11-28T00:28:03.315047Z","iopub.status.idle":"2021-11-28T00:28:03.32665Z","shell.execute_reply.started":"2021-11-28T00:28:03.315011Z","shell.execute_reply":"2021-11-28T00:28:03.325832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b5_c1.compile(loss='categorical_crossentropy',\n                optimizer='adam',\n                metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\n\nb5_c1_results = pretrained.fit_generator(train_generator,\n                                         steps_per_epoch=2699/20,# number of samples / batch size\n                                         epochs=20,\n                                        callbacks= early_stop2,\n                                         validation_data=test_generator)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T20:52:58.485468Z","iopub.execute_input":"2021-11-27T20:52:58.485735Z","iopub.status.idle":"2021-11-27T21:03:30.179699Z","shell.execute_reply.started":"2021-11-27T20:52:58.485705Z","shell.execute_reply":"2021-11-27T21:03:30.178812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_training_results(b5_c1_results)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T21:26:17.869304Z","iopub.execute_input":"2021-11-27T21:26:17.870069Z","iopub.status.idle":"2021-11-27T21:27:09.707814Z","shell.execute_reply.started":"2021-11-27T21:26:17.870012Z","shell.execute_reply":"2021-11-27T21:27:09.70711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Analysis of Model**\n\nThe best epoch had a training accuracy of ~ 96% and a testing accuracy of ~ 76%, with a training loss of ~ 12% and a testing loss of ~ 155%. It has a testing recall of 75%. These results are very similar to the previous model, so it looks like unfreezing a layer did not improve the model much. Will see if unfreezing one more layer will help.","metadata":{}},{"cell_type":"markdown","source":"### **Analysis of Model**\n\n","metadata":{}},{"cell_type":"markdown","source":"## **Unfreezing Two layers from Pretrained Nework (VGG)**","metadata":{}},{"cell_type":"code","source":"# Build first model using pretrained VGG 19 as first layer, and then some dense layers on top\nb5_c1c2 = keras.Sequential()\nb5_c1c2.add(cnn_vgg)\nb5_c1c2.add(layers.Flatten())\nb5_c1c2.add(layers.Dense(128, activation='relu'))\nb5_c1c2.add(layers.Dense(4, activation='softmax'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Re-freezing everything except for the last layer of the pretrained CNN\n# Code structure from https://github.com/learn-co-curriculum/dsc-using-pretrained-networks-codealong\ncnn_vgg.trainable = True\nfor layer in  cnn_vgg.layers:\n    if (layer.name == 'block5_conv1') | (layer.name == 'block5_conv2'):\n        layer.trainable = True\n    else:\n        layer.trainable = False","metadata":{"execution":{"iopub.status.busy":"2021-11-27T19:41:10.237425Z","iopub.execute_input":"2021-11-27T19:41:10.237964Z","iopub.status.idle":"2021-11-27T19:41:10.244567Z","shell.execute_reply.started":"2021-11-27T19:41:10.237923Z","shell.execute_reply":"2021-11-27T19:41:10.243617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking to see that only 'block5_conv1' and 'block5_conv3' layers are unfrozen\nfor layer in cnn_vgg.layers:\n    print(layer.name, layer.trainable)\n    \nprint(len(cnn_vgg.trainable_weights))","metadata":{"execution":{"iopub.status.busy":"2021-11-27T19:41:11.905509Z","iopub.execute_input":"2021-11-27T19:41:11.906211Z","iopub.status.idle":"2021-11-27T19:41:11.91963Z","shell.execute_reply.started":"2021-11-27T19:41:11.906167Z","shell.execute_reply":"2021-11-27T19:41:11.918731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b5_c1c2.compile(loss='categorical_crossentropy',\n                optimizer='adam',\n                metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\n\nb5_c1c2_results = pretrained.fit_generator(train_generator,\n                                         steps_per_epoch=2699/20,# number of samples / batch size\n                                         epochs=20,\n                                        callbacks= early_stop2,\n                                         validation_data=test_generator)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T19:41:57.522447Z","iopub.execute_input":"2021-11-27T19:41:57.522702Z","iopub.status.idle":"2021-11-27T19:56:38.7073Z","shell.execute_reply.started":"2021-11-27T19:41:57.522674Z","shell.execute_reply":"2021-11-27T19:56:38.70658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_training_results(b5_c1c2_results)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T19:56:59.185163Z","iopub.execute_input":"2021-11-27T19:56:59.185428Z","iopub.status.idle":"2021-11-27T19:56:59.492264Z","shell.execute_reply.started":"2021-11-27T19:56:59.185399Z","shell.execute_reply":"2021-11-27T19:56:59.491584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Analysis of Model**\n\nThe epoch with the lowest loss has a training accuracy of 93% and a testing accuracy of 76%, with a training loss of 18% and a testing loss of 118%. Testing recall is 69%. It looks like unfreezing two layers has helped decrease the testing a loss a bit; without any unfrozen layers, testing loss is around 184%. ","metadata":{}},{"cell_type":"markdown","source":"## **Unfreezing all Four Convolutional Layers in Block 5 of Pretrained Neural Network**","metadata":{}},{"cell_type":"code","source":"# Build first model using pretrained VGG 19 as first layer, and then some dense layers on top\nb5_c1c2c3c4 = keras.Sequential()\nb5_c1c2c3c4.add(cnn_vgg)\nb5_c1c2c3c4.add(layers.Flatten())\nb5_c1c2c3c4.add(layers.Dense(128, activation='relu'))\nb5_c1c2c3c4.add(layers.Dense(4, activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2021-11-27T21:41:09.928699Z","iopub.execute_input":"2021-11-27T21:41:09.929412Z","iopub.status.idle":"2021-11-27T21:41:10.013489Z","shell.execute_reply.started":"2021-11-27T21:41:09.929372Z","shell.execute_reply":"2021-11-27T21:41:10.012701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Re-freezing everything except for the last layer of the pretrained CNN\n# Code structure from https://github.com/learn-co-curriculum/dsc-using-pretrained-networks-codealong\ncnn_vgg.trainable = True\nfor layer in  cnn_vgg.layers:\n    if (layer.name == 'block5_conv1') | (layer.name == 'block5_conv2') | (layer.name == 'block5_conv3') | (layer.name == 'block5_conv4'):\n        layer.trainable = True\n    else:\n        layer.trainable = False","metadata":{"execution":{"iopub.status.busy":"2021-11-27T21:41:54.085305Z","iopub.execute_input":"2021-11-27T21:41:54.085847Z","iopub.status.idle":"2021-11-27T21:41:54.092438Z","shell.execute_reply.started":"2021-11-27T21:41:54.085808Z","shell.execute_reply":"2021-11-27T21:41:54.091559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking to see that only 'block5_conv1' and 'block5_conv3' layers are unfrozen\nfor layer in cnn_vgg.layers:\n    print(layer.name, layer.trainable)\n    \nprint(len(cnn_vgg.trainable_weights))","metadata":{"execution":{"iopub.status.busy":"2021-11-28T00:26:38.275062Z","iopub.execute_input":"2021-11-28T00:26:38.275351Z","iopub.status.idle":"2021-11-28T00:26:38.300793Z","shell.execute_reply.started":"2021-11-28T00:26:38.275316Z","shell.execute_reply":"2021-11-28T00:26:38.299752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b5_c1c2c3c4.compile(loss='categorical_crossentropy',\n                optimizer='adam',\n                metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\n\nb5_c1c2c3c4_results = pretrained.fit_generator(train_generator,\n                                         steps_per_epoch=2699/20,# number of samples / batch size\n                                         epochs=20,\n                                        callbacks= early_stop2,\n                                         validation_data=test_generator)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T21:42:57.188876Z","iopub.execute_input":"2021-11-27T21:42:57.189609Z","iopub.status.idle":"2021-11-27T21:58:31.706354Z","shell.execute_reply.started":"2021-11-27T21:42:57.189567Z","shell.execute_reply":"2021-11-27T21:58:31.705408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Analysis of Model**\n\nThe epoch with the lowest loss has a training accuracy of 97% and a testing accuracy of 77%, with a training loss of 8% and a testing loss of 173%. Testing recall is 76%. Testing loss seems to have increased from the last model iteration which only had two layers unfrozen, so I will return to just unfreezing two layers in the iteration, and will also try seeing if batch normalization and dropout layers improve the model.","metadata":{}},{"cell_type":"markdown","source":"## **Adding Batch Normalization and Dropout layers to Model with Two Unfrozen Layers**","metadata":{}},{"cell_type":"code","source":"# Build first model using pretrained VGG 19 as first layer, and then some dense layers on top\nn_b5_c1c2 = keras.Sequential()\nn_b5_c1c2.add(cnn_vgg)\nn_b5_c1c2.add(layers.Dropout(0.25))\nn_b5_c1c2.add(layers.BatchNormalization())\n\nn_b5_c1c2.add(layers.Flatten())\nn_b5_c1c2.add(layers.Dense(128, activation='relu'))\nn_b5_c1c2.add(layers.Dropout(0.3))\nn_b5_c1c2.add(layers.BatchNormalization())\nn_b5_c1c2.add(layers.Dense(4, activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2021-11-27T22:14:08.294255Z","iopub.execute_input":"2021-11-27T22:14:08.294535Z","iopub.status.idle":"2021-11-27T22:14:08.402485Z","shell.execute_reply.started":"2021-11-27T22:14:08.294503Z","shell.execute_reply":"2021-11-27T22:14:08.401786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Re-freezing everything except for the last layer of the pretrained CNN\n# Code structure from https://github.com/learn-co-curriculum/dsc-using-pretrained-networks-codealong\ncnn_vgg.trainable = True\nfor layer in  cnn_vgg.layers:\n    if (layer.name == 'block5_conv1') | (layer.name == 'block5_conv2'):\n        layer.trainable = True\n    else:\n        layer.trainable = False","metadata":{"execution":{"iopub.status.busy":"2021-11-27T22:14:13.017696Z","iopub.execute_input":"2021-11-27T22:14:13.018002Z","iopub.status.idle":"2021-11-27T22:14:13.025167Z","shell.execute_reply.started":"2021-11-27T22:14:13.017969Z","shell.execute_reply":"2021-11-27T22:14:13.024316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking to see that only 'block5_conv1' and 'block5_conv3' layers are unfrozen\nfor layer in cnn_vgg.layers:\n    print(layer.name, layer.trainable)\n    \nprint(len(cnn_vgg.trainable_weights))","metadata":{"execution":{"iopub.status.busy":"2021-11-27T22:14:15.46644Z","iopub.execute_input":"2021-11-27T22:14:15.466733Z","iopub.status.idle":"2021-11-27T22:14:15.480782Z","shell.execute_reply.started":"2021-11-27T22:14:15.466701Z","shell.execute_reply":"2021-11-27T22:14:15.479716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_b5_c1c2.compile(loss='categorical_crossentropy',\n                optimizer='adam',\n                metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\n\nn_b5_c1c2_results = pretrained.fit_generator(train_generator,\n                                         steps_per_epoch=2699/20,# number of samples / batch size\n                                         epochs=20,\n                                        callbacks= early_stop2,\n                                         validation_data=test_generator)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T22:14:23.052498Z","iopub.execute_input":"2021-11-27T22:14:23.052757Z","iopub.status.idle":"2021-11-27T22:28:56.321315Z","shell.execute_reply.started":"2021-11-27T22:14:23.052728Z","shell.execute_reply":"2021-11-27T22:28:56.320555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_training_results(n_b5_c1c2_results)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T22:34:51.974286Z","iopub.execute_input":"2021-11-27T22:34:51.974768Z","iopub.status.idle":"2021-11-27T22:34:52.272677Z","shell.execute_reply.started":"2021-11-27T22:34:51.974728Z","shell.execute_reply":"2021-11-27T22:34:52.27191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Analysis of Model**\n\nThe epoch with the lowest testing loss had a training accuracy of around 97% and a testing accuracy of 78%, with a training loss of 7% and a testing loss of 151%. Testing recall is 77%. Testing loss is slightly better than in the last model, but overall it is not much better. Maybe a different pre-trained network would be better.","metadata":{}},{"cell_type":"markdown","source":"## **Using Pretrained Weights from VGG-16**","metadata":{}},{"cell_type":"code","source":"from keras.applications.vgg16 import VGG16\nvgg16 = VGG16(weights='imagenet',\n               include_top=False,\n               input_shape=(200,200,3))","metadata":{"execution":{"iopub.status.busy":"2021-11-28T02:26:27.581689Z","iopub.execute_input":"2021-11-28T02:26:27.581938Z","iopub.status.idle":"2021-11-28T02:26:27.854068Z","shell.execute_reply.started":"2021-11-28T02:26:27.581909Z","shell.execute_reply":"2021-11-28T02:26:27.853287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vgg16.summary()","metadata":{"execution":{"iopub.status.busy":"2021-11-28T02:26:29.806481Z","iopub.execute_input":"2021-11-28T02:26:29.806753Z","iopub.status.idle":"2021-11-28T02:26:29.823456Z","shell.execute_reply.started":"2021-11-28T02:26:29.806723Z","shell.execute_reply":"2021-11-28T02:26:29.822574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build first model using pretrained VGG 19 as first layer, and then some dense layers on top\nvgg16_model = keras.Sequential()\nvgg16_model.add(vgg16)\nvgg16_model.add(layers.Flatten())\nvgg16_model.add(layers.Dense(128, activation='relu'))\nvgg16_model.add(layers.Dense(4, activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2021-11-28T02:26:32.719853Z","iopub.execute_input":"2021-11-28T02:26:32.720395Z","iopub.status.idle":"2021-11-28T02:26:32.792477Z","shell.execute_reply.started":"2021-11-28T02:26:32.720354Z","shell.execute_reply":"2021-11-28T02:26:32.791779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vgg16.trainable = False","metadata":{"execution":{"iopub.status.busy":"2021-11-28T02:26:34.895959Z","iopub.execute_input":"2021-11-28T02:26:34.896775Z","iopub.status.idle":"2021-11-28T02:26:34.900891Z","shell.execute_reply.started":"2021-11-28T02:26:34.896735Z","shell.execute_reply":"2021-11-28T02:26:34.900146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check to see that the pretrained layer is not trainable but that all others are\nfor layer in vgg16_model.layers:\n    print(layer.name, layer.trainable)\n    \nprint(len(vgg16_model.trainable_weights))","metadata":{"execution":{"iopub.status.busy":"2021-11-28T02:26:37.876728Z","iopub.execute_input":"2021-11-28T02:26:37.877263Z","iopub.status.idle":"2021-11-28T02:26:37.883896Z","shell.execute_reply.started":"2021-11-28T02:26:37.877223Z","shell.execute_reply":"2021-11-28T02:26:37.88319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vgg16_model.compile(optimizer='adam',\n                    loss='categorical_crossentropy',\n                    metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-28T02:26:43.920857Z","iopub.execute_input":"2021-11-28T02:26:43.921549Z","iopub.status.idle":"2021-11-28T02:26:43.93189Z","shell.execute_reply.started":"2021-11-28T02:26:43.921508Z","shell.execute_reply":"2021-11-28T02:26:43.930982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vgg16_model_results = vgg16_model.fit_generator(train_generator,\n                                         steps_per_epoch=2699/20,# number of samples / batch size\n                                         epochs=20,\n                                        callbacks= early_stop2,\n                                         validation_data=test_generator)","metadata":{"execution":{"iopub.status.busy":"2021-11-28T02:26:47.327683Z","iopub.execute_input":"2021-11-28T02:26:47.328157Z","iopub.status.idle":"2021-11-28T02:38:21.293549Z","shell.execute_reply.started":"2021-11-28T02:26:47.32812Z","shell.execute_reply":"2021-11-28T02:38:21.292821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Analysis of Model**\n\nThe epoch with the lowest testing loss had a training accuracy of 86% and a testing accuracy of 68%, with a training loss of 37% and a testing loss of 110%. Testing recall is 65%. Testing loss is slightly lower than the previous model using VGG19, but accuracy and recall is significantly lower, so it looks like using the VGG network with more layers is better. However, it is still a good idea to continue to try different pretrained networks.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.applications.inception_v3 import InceptionV3\nincep = InceptionV3(weights='imagenet',\n               include_top=False,\n               input_shape=(200,200,3))","metadata":{"execution":{"iopub.status.busy":"2021-11-28T04:42:43.196132Z","iopub.execute_input":"2021-11-28T04:42:43.196394Z","iopub.status.idle":"2021-11-28T04:42:45.973591Z","shell.execute_reply.started":"2021-11-28T04:42:43.196362Z","shell.execute_reply":"2021-11-28T04:42:45.97272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"incep.summary()\n","metadata":{"execution":{"iopub.status.busy":"2021-11-28T02:08:23.682043Z","iopub.execute_input":"2021-11-28T02:08:23.682773Z","iopub.status.idle":"2021-11-28T02:08:23.829795Z","shell.execute_reply.started":"2021-11-28T02:08:23.682717Z","shell.execute_reply":"2021-11-28T02:08:23.828983Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build first model using pretrained VGG 19 as first layer, and then some dense layers on top\nincep_model = keras.Sequential()\nincep_model.add(incep)\nincep_model.add(layers.Flatten())\nincep_model.add(layers.Dense(512, activation='relu'))\nincep_model.add(layers.Dense(4, activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2021-11-28T02:09:36.676467Z","iopub.execute_input":"2021-11-28T02:09:36.676722Z","iopub.status.idle":"2021-11-28T02:09:37.239399Z","shell.execute_reply.started":"2021-11-28T02:09:36.676692Z","shell.execute_reply":"2021-11-28T02:09:37.236561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"incep.trainable = False","metadata":{"execution":{"iopub.status.busy":"2021-11-28T02:09:39.644397Z","iopub.execute_input":"2021-11-28T02:09:39.644656Z","iopub.status.idle":"2021-11-28T02:09:39.661314Z","shell.execute_reply.started":"2021-11-28T02:09:39.644621Z","shell.execute_reply":"2021-11-28T02:09:39.660295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check to see that the pretrained layer is not trainable but that all others are\nfor layer in incep_model.layers:\n    print(layer.name, layer.trainable)\n    \nprint(len(incep_model.trainable_weights))","metadata":{"execution":{"iopub.status.busy":"2021-11-28T02:10:02.751351Z","iopub.execute_input":"2021-11-28T02:10:02.751623Z","iopub.status.idle":"2021-11-28T02:10:02.759079Z","shell.execute_reply.started":"2021-11-28T02:10:02.751587Z","shell.execute_reply":"2021-11-28T02:10:02.758282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"incep_model.compile(optimizer='adam',\n                    loss='categorical_crossentropy',\n                    metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\n\nincep_model_results = incep_model.fit_generator(train_generator,\n                                         steps_per_epoch=2699/20,# number of samples / batch size\n                                         epochs=20,\n                                        callbacks= early_stop2,\n                                         validation_data=test_generator)","metadata":{"execution":{"iopub.status.busy":"2021-11-28T02:10:54.69381Z","iopub.execute_input":"2021-11-28T02:10:54.694079Z","iopub.status.idle":"2021-11-28T02:24:48.926051Z","shell.execute_reply.started":"2021-11-28T02:10:54.69405Z","shell.execute_reply":"2021-11-28T02:24:48.925332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_training_results(incep_model_results)","metadata":{"execution":{"iopub.status.busy":"2021-11-28T02:26:09.781184Z","iopub.execute_input":"2021-11-28T02:26:09.781652Z","iopub.status.idle":"2021-11-28T02:26:10.086771Z","shell.execute_reply.started":"2021-11-28T02:26:09.781615Z","shell.execute_reply":"2021-11-28T02:26:10.085947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Analysis of Model**\n\nThe epoch with the lowest testing loss has a training accuracy of 91% and a testing accuracy of 75%, with a training loss of 25% and a testing loss of 90%. Testing recall is around 73% This pretrained network resulted in the lowest testing loss yet, which is very promising; in the next few iterations I will try fine tuning this inception pretrained network.","metadata":{}},{"cell_type":"markdown","source":"## **Fine Tuning of Inception Network**\nAdding class weights to account for class imbalance, adding batch normalization and dropout layer, unfreezing outer most convolution layer of Inception Network","metadata":{}},{"cell_type":"code","source":"multi_weights = {0: 1,\n                1: 1 ,\n                2: 2,\n                3: 1,}\n# Add class weights\n# unfreeze outer layer\n# decrease learning rate\n","metadata":{"execution":{"iopub.status.busy":"2021-11-28T04:40:48.477896Z","iopub.execute_input":"2021-11-28T04:40:48.478641Z","iopub.status.idle":"2021-11-28T04:40:48.483003Z","shell.execute_reply.started":"2021-11-28T04:40:48.478604Z","shell.execute_reply":"2021-11-28T04:40:48.482106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build first model using pretrained VGG 19 as first layer, and then some dense layers on top\nincep2_model = keras.Sequential()\nincep2_model.add(incep)\nincep2_model.add(layers.Flatten())\nincep2_model.add(layers.Dense(512, activation='relu'))\nincep2_model.add(layers.BatchNormalization())\nincep2_model.add(layers.Dropout(0.3))\nincep2_model.add(layers.Dense(4, activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2021-11-28T02:59:26.858707Z","iopub.execute_input":"2021-11-28T02:59:26.858995Z","iopub.status.idle":"2021-11-28T02:59:27.4256Z","shell.execute_reply.started":"2021-11-28T02:59:26.858968Z","shell.execute_reply":"2021-11-28T02:59:27.424772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"incep.trainable = False\n","metadata":{"execution":{"iopub.status.busy":"2021-11-28T02:59:37.016358Z","iopub.execute_input":"2021-11-28T02:59:37.016948Z","iopub.status.idle":"2021-11-28T02:59:37.034819Z","shell.execute_reply.started":"2021-11-28T02:59:37.016909Z","shell.execute_reply":"2021-11-28T02:59:37.033978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check to see that the pretrained layer is not trainable but that all others are\nfor layer in incep2_model.layers:\n    print(layer.name, layer.trainable)\n    \nprint(len(incep2_model.trainable_weights))","metadata":{"execution":{"iopub.status.busy":"2021-11-28T02:59:39.346504Z","iopub.execute_input":"2021-11-28T02:59:39.347294Z","iopub.status.idle":"2021-11-28T02:59:39.355614Z","shell.execute_reply.started":"2021-11-28T02:59:39.347228Z","shell.execute_reply":"2021-11-28T02:59:39.354653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Re-freezing everything except for the last layer of the pretrained CNN\n# Code structure from https://github.com/learn-co-curriculum/dsc-using-pretrained-networks-codealong\nincep.trainable = True\nfor layer in  incep.layers:\n    if (layer.name == 'conv2d_93'):\n        layer.trainable = True\n    else:\n        layer.trainable = False","metadata":{"execution":{"iopub.status.busy":"2021-11-28T02:59:41.922134Z","iopub.execute_input":"2021-11-28T02:59:41.922409Z","iopub.status.idle":"2021-11-28T02:59:41.948334Z","shell.execute_reply.started":"2021-11-28T02:59:41.922378Z","shell.execute_reply":"2021-11-28T02:59:41.947471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for layer in incep.layers:\n    print(layer.name, layer.trainable)\n    \nprint(len(incep.trainable_weights))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-11-28T02:59:45.660062Z","iopub.execute_input":"2021-11-28T02:59:45.660493Z","iopub.status.idle":"2021-11-28T02:59:45.760507Z","shell.execute_reply.started":"2021-11-28T02:59:45.660457Z","shell.execute_reply":"2021-11-28T02:59:45.759778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multi_weights = {0: 1,\n                1: 1 ,\n                2: 2,\n                3: 1,}\n\nincep2_model.compile(optimizer='adam',\n                    loss='categorical_crossentropy',\n                    metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\n\nincep2_model_results = incep2_model.fit_generator(train_generator,\n                                        class_weight= multi_weights,\n                                         steps_per_epoch=2699/20,# number of samples / batch size\n                                         epochs=20,\n                                        callbacks= early_stop2,\n                                         validation_data=test_generator)","metadata":{"execution":{"iopub.status.busy":"2021-11-28T03:00:11.313787Z","iopub.execute_input":"2021-11-28T03:00:11.314351Z","iopub.status.idle":"2021-11-28T03:14:28.161343Z","shell.execute_reply.started":"2021-11-28T03:00:11.314311Z","shell.execute_reply":"2021-11-28T03:14:28.160502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_training_results(incep2_model_results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Analysis of Model**\n\nThe epoch with the lowest testing loss had a training accuracy of 94% and a testing accuracy of 77%, with a training loss of 16% and a testing loss of 105%. Testing recall is 69%. Accuracy between this iteration and the last is similar, but testing loss is about ten percentage points higher, so the model still requires some tuning. ","metadata":{}},{"cell_type":"markdown","source":"## **Model with Inception Pretrained Network using a smaller learning rate**","metadata":{}},{"cell_type":"code","source":"# Build first model using pretrained VGG 19 as first layer, and then some dense layers on top\nincep3_model = keras.Sequential()\nincep3_model.add(incep)\nincep3_model.add(layers.Flatten())\nincep3_model.add(layers.Dense(512, activation='relu'))\nincep3_model.add(layers.BatchNormalization())\nincep3_model.add(layers.Dropout(0.3))\nincep3_model.add(layers.Dense(4, activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2021-11-28T04:42:59.766128Z","iopub.execute_input":"2021-11-28T04:42:59.766385Z","iopub.status.idle":"2021-11-28T04:43:00.318713Z","shell.execute_reply.started":"2021-11-28T04:42:59.766356Z","shell.execute_reply":"2021-11-28T04:43:00.31799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"incep.trainable = False","metadata":{"execution":{"iopub.status.busy":"2021-11-28T04:43:03.420039Z","iopub.execute_input":"2021-11-28T04:43:03.420726Z","iopub.status.idle":"2021-11-28T04:43:03.435774Z","shell.execute_reply.started":"2021-11-28T04:43:03.420688Z","shell.execute_reply":"2021-11-28T04:43:03.434958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check to see that the pretrained layer is not trainable but that all others are\nfor layer in incep3_model.layers:\n    print(layer.name, layer.trainable)\n    \nprint(len(incep3_model.trainable_weights))","metadata":{"execution":{"iopub.status.busy":"2021-11-28T04:43:13.345102Z","iopub.execute_input":"2021-11-28T04:43:13.345629Z","iopub.status.idle":"2021-11-28T04:43:13.354338Z","shell.execute_reply.started":"2021-11-28T04:43:13.345589Z","shell.execute_reply":"2021-11-28T04:43:13.353457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Re-freezing everything except for the last layer of the pretrained CNN\n# Code structure from https://github.com/learn-co-curriculum/dsc-using-pretrained-networks-codealong\nincep.trainable = True\nfor layer in  incep.layers:\n    if (layer.name == 'conv2d_93'):\n        layer.trainable = True\n    else:\n        layer.trainable = False","metadata":{"execution":{"iopub.status.busy":"2021-11-28T04:44:26.975113Z","iopub.execute_input":"2021-11-28T04:44:26.975377Z","iopub.status.idle":"2021-11-28T04:44:27.000423Z","shell.execute_reply.started":"2021-11-28T04:44:26.975347Z","shell.execute_reply":"2021-11-28T04:44:26.999472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for layer in incep.layers:\n    print(layer.name, layer.trainable)\n    \nprint(len(incep.trainable_weights))","metadata":{"execution":{"iopub.status.busy":"2021-11-28T04:44:34.378645Z","iopub.execute_input":"2021-11-28T04:44:34.3795Z","iopub.status.idle":"2021-11-28T04:44:34.470095Z","shell.execute_reply.started":"2021-11-28T04:44:34.379455Z","shell.execute_reply":"2021-11-28T04:44:34.469428Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multi_weights = {0: 1,\n                1: 1 ,\n                2: 2,\n                3: 1,}\n\nsmall_lr_adam = keras.optimizers.Adam(learning_rate=0.0001)\n\nincep3_model.compile(optimizer= small_lr_adam,\n                    loss='categorical_crossentropy',\n                    metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\n\nincep3_model_results = incep3_model.fit_generator(train_generator,\n                                        class_weight= multi_weights,\n                                         steps_per_epoch=2699/20,# number of samples / batch size\n                                         epochs=20,\n                                        callbacks= early_stop2,\n                                         validation_data=test_generator)","metadata":{"execution":{"iopub.status.busy":"2021-11-28T04:46:05.196424Z","iopub.execute_input":"2021-11-28T04:46:05.196991Z","iopub.status.idle":"2021-11-28T05:01:53.675172Z","shell.execute_reply.started":"2021-11-28T04:46:05.196955Z","shell.execute_reply":"2021-11-28T05:01:53.674429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_training_results(incep3_model_results)","metadata":{"execution":{"iopub.status.busy":"2021-11-28T05:02:13.667479Z","iopub.execute_input":"2021-11-28T05:02:13.668022Z","iopub.status.idle":"2021-11-28T05:02:13.957048Z","shell.execute_reply.started":"2021-11-28T05:02:13.667985Z","shell.execute_reply":"2021-11-28T05:02:13.956415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Analysis of Model**\n\nThe epoch with the lowest testing loss had a training accuracy of 94% and a testing acuracy of 76%, with a training loss of 17% and a testing loss of around 99%; the model is still overfitting. Tetsing recall is 71%. Other than the testing loss being slightly lower in this iteration than the last, the numbers between this iteration and the ast are fairly similar; it is possible this model could improve with more training epochs. It is also possible that unfreezing more layers from the pretrained base might help the network better learn and adjust to these specific images.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}