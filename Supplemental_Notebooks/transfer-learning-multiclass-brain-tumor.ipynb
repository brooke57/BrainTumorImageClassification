{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"execution":{"iopub.status.busy":"2022-02-08T03:33:20.325836Z","iopub.execute_input":"2022-02-08T03:33:20.326448Z","iopub.status.idle":"2022-02-08T03:33:21.499271Z","shell.execute_reply.started":"2022-02-08T03:33:20.326413Z","shell.execute_reply":"2022-02-08T03:33:21.498580Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Re-freezing everything except for the last layer of the pretrained CNN\n# Code structure from https://github.com/learn-co-curriculum/dsc-using-pretrained-networks-codealong\ndef Unfreeze_Layers(pretrain, layer_list):\n    pretrain.trainable = True\n    for layer in  pretrain.layers:\n        if layer.name in layer_list:\n            layer.trainable = True\n        else:\n            layer.trainable = False\n        \n    for layer in pretrain.layers:\n        print(layer.name, layer.trainable)\n    print(len(pretrain.trainable_weights))","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:33:21.500939Z","iopub.execute_input":"2022-02-08T03:33:21.501200Z","iopub.status.idle":"2022-02-08T03:33:21.506885Z","shell.execute_reply.started":"2022-02-08T03:33:21.501164Z","shell.execute_reply":"2022-02-08T03:33:21.506060Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def Freeze_Pretrained_Base(pretrain, network):\n    pretrain.trainable = False\n    for layer in network.layers:\n        print(layer.name, layer.trainable)\n    print(len(network.trainable_weights))","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:33:21.508556Z","iopub.execute_input":"2022-02-08T03:33:21.508875Z","iopub.status.idle":"2022-02-08T03:33:21.515303Z","shell.execute_reply.started":"2022-02-08T03:33:21.508840Z","shell.execute_reply":"2022-02-08T03:33:21.514540Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def visualize_training_results(history):\n    '''\n    From https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n    \n    Input: keras history object (output from trained model)\n    '''\n    fig, (ax1, ax2) = plt.subplots(2, sharex=True)\n    fig.suptitle('Model Results')\n\n    # summarize history for accuracy\n    ax1.plot(history.history['acc'])\n    ax1.plot(history.history['val_acc'])\n    ax1.set_ylabel('Accuracy')\n    ax1.legend(['train', 'test'], loc='upper left')\n    # summarize history for loss\n    ax2.plot(history.history['loss'])\n    ax2.plot(history.history['val_loss'])\n    ax2.set_ylabel('Loss')\n    ax2.legend(['train', 'test'], loc='upper left')\n    \n    plt.xlabel('Epoch')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:33:24.866049Z","iopub.execute_input":"2022-02-08T03:33:24.866329Z","iopub.status.idle":"2022-02-08T03:33:24.875298Z","shell.execute_reply.started":"2022-02-08T03:33:24.866298Z","shell.execute_reply":"2022-02-08T03:33:24.872689Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def print_metrics(model_history):\n\n    metrics = ['val_loss', 'val_acc', 'val_recall', 'val_precision', 'val_true_positives', 'val_true_negatives', 'val_false_positives', 'val_false_negatives']\n    tp = model_history['val_true_positives'][-1]\n    fp = model_history['val_false_positives'][-1]\n    fn = model_history['val_false_negatives'][-1]\n    f1 = tp/(tp+(0.5*(fp+fn)))\n    for x in metrics:\n        print(x+':',model_history[x][-1])\n    print('F1 score:',f1)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:33:27.108180Z","iopub.execute_input":"2022-02-08T03:33:27.108449Z","iopub.status.idle":"2022-02-08T03:33:27.114542Z","shell.execute_reply.started":"2022-02-08T03:33:27.108420Z","shell.execute_reply":"2022-02-08T03:33:27.113797Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Set random state for numpy operations\nfrom numpy.random import seed\nseed(2)\n# Set random state for tensorflow operations\nfrom tensorflow.random import set_seed\nset_seed(3)\n# General imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import regularizers\nfrom keras.regularizers import l2\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.models import load_model\nimport seaborn as sns\nfrom mlxtend.plotting import plot_decision_regions\nfrom sklearn.metrics import multilabel_confusion_matrix, ConfusionMatrixDisplay\nimport cv2\nimport PIL","metadata":{"execution":{"iopub.status.busy":"2022-02-08T04:29:52.088669Z","iopub.execute_input":"2022-02-08T04:29:52.088944Z","iopub.status.idle":"2022-02-08T04:29:52.179488Z","shell.execute_reply.started":"2022-02-08T04:29:52.088916Z","shell.execute_reply":"2022-02-08T04:29:52.178554Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"# Set up ImageDataGenerator\ntrain_imagegen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255,\n                                   zoom_range=[0.6,1],\n                                   rotation_range=10,\n                                   brightness_range=([0.6, 1.2]),\n                                   horizontal_flip=True,\n                                   validation_split=0.06) # this will set aside a part of training set for validation data\ntest_imagegen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255,\n                                   zoom_range=[0.6,1],\n                                   rotation_range=10,\n                                   brightness_range=([0.6, 1.2]),\n                                   horizontal_flip=True)\n# Bring the data in\ntrain_generator = train_imagegen.flow_from_directory(\n                                    '../input/brain-tumor-classification-mri/Training',\n                                    classes={'no_tumor': 0,\n                                            'glioma_tumor':1,\n                                            'meningioma_tumor':2,\n                                            'pituitary_tumor':3},\n                                    target_size=(150,150),\n                                    batch_size=2700,# number of training images\n                                    seed=42,\n                                    class_mode='categorical',\n                                    subset='training')\n\ntest_generator = test_imagegen.flow_from_directory(\n                                    '../input/brain-tumor-classification-mri/Testing',\n                                    classes={'no_tumor': 0,\n                                            'glioma_tumor':1,\n                                            'meningioma_tumor':2,\n                                            'pituitary_tumor':3},\n                                    target_size=(150,150),\n                                    batch_size=394,# number of images\n                                    seed=42,\n                                    class_mode='categorical')\n\nval_generator = train_imagegen.flow_from_directory(\n                                    '../input/brain-tumor-classification-mri/Training',\n                                    classes={'no_tumor': 0,\n                                            'glioma_tumor':1,\n                                            'meningioma_tumor':2,\n                                            'pituitary_tumor':3},\n                                    target_size=(150,150),\n                                    batch_size=170,# number of images\n                                    seed=42,\n                                    class_mode='categorical',\n                                    subset='validation')\n# First run-throughs were not done with a random seed, so model analysis may be slightly different from what will be the \n# actual numbers after running models with the random seed.","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:33:59.494498Z","iopub.execute_input":"2022-02-08T03:33:59.494755Z","iopub.status.idle":"2022-02-08T03:33:59.920363Z","shell.execute_reply.started":"2022-02-08T03:33:59.494728Z","shell.execute_reply":"2022-02-08T03:33:59.919623Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# # Creating variables to contain image vectors and labels for the different training sets\ntrain_img, train_lab = next(train_generator)\ntest_img, test_lab = next(test_generator)\nval_img, val_lab = next(val_generator)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:34:02.388627Z","iopub.execute_input":"2022-02-08T03:34:02.389007Z","iopub.status.idle":"2022-02-08T03:34:44.640566Z","shell.execute_reply.started":"2022-02-08T03:34:02.388972Z","shell.execute_reply":"2022-02-08T03:34:44.639817Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## **Using Pre-Trained VGG-19 Weights**","metadata":{}},{"cell_type":"code","source":"from keras.applications.vgg19 import VGG19\ncnn_vgg = VGG19(weights='imagenet',\n               include_top=False,\n               input_shape=(150,150,3))","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:34:47.411701Z","iopub.execute_input":"2022-02-08T03:34:47.412308Z","iopub.status.idle":"2022-02-08T03:34:50.415258Z","shell.execute_reply.started":"2022-02-08T03:34:47.412251Z","shell.execute_reply":"2022-02-08T03:34:50.414499Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Making early stop for model\npre_early = [EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True),\n            ModelCheckpoint(filepath='pretrained_model.h5', monitor='val_loss',\n                           save_best_only=True)]","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:36:00.783005Z","iopub.execute_input":"2022-02-08T03:36:00.783666Z","iopub.status.idle":"2022-02-08T03:36:00.788855Z","shell.execute_reply.started":"2022-02-08T03:36:00.783629Z","shell.execute_reply":"2022-02-08T03:36:00.788140Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"cnn_vgg.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:36:03.160186Z","iopub.execute_input":"2022-02-08T03:36:03.160593Z","iopub.status.idle":"2022-02-08T03:36:03.180461Z","shell.execute_reply.started":"2022-02-08T03:36:03.160559Z","shell.execute_reply":"2022-02-08T03:36:03.179765Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Build first model using pretrained VGG 19 as first layer, and then some dense layers on top\npretrained = keras.Sequential()\npretrained.add(cnn_vgg)\npretrained.add(layers.Flatten())\npretrained.add(layers.Dense(128, activation='relu'))\npretrained.add(layers.Dense(4, activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:36:06.940513Z","iopub.execute_input":"2022-02-08T03:36:06.941059Z","iopub.status.idle":"2022-02-08T03:36:07.018430Z","shell.execute_reply.started":"2022-02-08T03:36:06.941024Z","shell.execute_reply":"2022-02-08T03:36:07.017758Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"Freeze_Pretrained_Base(cnn_vgg, pretrained)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:36:11.763826Z","iopub.execute_input":"2022-02-08T03:36:11.764091Z","iopub.status.idle":"2022-02-08T03:36:11.772002Z","shell.execute_reply.started":"2022-02-08T03:36:11.764050Z","shell.execute_reply":"2022-02-08T03:36:11.771291Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"\npretrained.compile(loss='categorical_crossentropy',\n                optimizer='adam',\n                metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\n\npretrained_results = pretrained.fit(x=train_img, y=train_lab,\n                                              batch_size = 32,\n                                         steps_per_epoch=2700//32+1,# number of samples / batch size\n                                         epochs=25,\n                                        callbacks= pre_early,\n                                         validation_data=(test_img, test_lab),\n                                        validation_steps = 394//32+1)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:36:20.819962Z","iopub.execute_input":"2022-02-08T03:36:20.820247Z","iopub.status.idle":"2022-02-08T03:37:38.613965Z","shell.execute_reply.started":"2022-02-08T03:36:20.820204Z","shell.execute_reply":"2022-02-08T03:37:38.613150Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"visualize_training_results(pretrained_results)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:37:54.778794Z","iopub.execute_input":"2022-02-08T03:37:54.779050Z","iopub.status.idle":"2022-02-08T03:37:55.071315Z","shell.execute_reply.started":"2022-02-08T03:37:54.779022Z","shell.execute_reply":"2022-02-08T03:37:55.070629Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"pretrained.evaluate(test_img, test_lab)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:37:59.138738Z","iopub.execute_input":"2022-02-08T03:37:59.139305Z","iopub.status.idle":"2022-02-08T03:37:59.993430Z","shell.execute_reply.started":"2022-02-08T03:37:59.139264Z","shell.execute_reply":"2022-02-08T03:37:59.992754Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":" loss: 1.266525387763977,\\\n acc: 0.5634517669677734,\\\n recall: 0.5025380849838257,\\\n precision: 0.5892857313156128,\\\n tp: 198.0,\\\n tn: 1044.0,\\\nfp: 138.0,\\\nfn: 196.0","metadata":{}},{"cell_type":"markdown","source":"## **Unfreezing an outer Layer of the Pretrained Network**","metadata":{}},{"cell_type":"code","source":"# Making early stop for model\nb5c1_early = [EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True),\n            ModelCheckpoint(filepath='b5_c1_model.h5', monitor='val_loss',\n                           save_best_only=True)]","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:38:11.356274Z","iopub.execute_input":"2022-02-08T03:38:11.356700Z","iopub.status.idle":"2022-02-08T03:38:11.364403Z","shell.execute_reply.started":"2022-02-08T03:38:11.356654Z","shell.execute_reply":"2022-02-08T03:38:11.363367Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Build first model using pretrained VGG 19 as first layer, and then some dense layers on top\nb5_c1 = keras.Sequential()\nb5_c1.add(cnn_vgg)\nb5_c1.add(layers.Flatten())\nb5_c1.add(layers.Dense(128, activation='relu'))\nb5_c1.add(layers.Dense(4, activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:38:14.996288Z","iopub.execute_input":"2022-02-08T03:38:14.996697Z","iopub.status.idle":"2022-02-08T03:38:15.093931Z","shell.execute_reply.started":"2022-02-08T03:38:14.996665Z","shell.execute_reply":"2022-02-08T03:38:15.093285Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# freezing everything \nFreeze_Pretrained_Base(cnn_vgg, b5_c1)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:38:18.926839Z","iopub.execute_input":"2022-02-08T03:38:18.927103Z","iopub.status.idle":"2022-02-08T03:38:18.933510Z","shell.execute_reply.started":"2022-02-08T03:38:18.927073Z","shell.execute_reply":"2022-02-08T03:38:18.932777Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Unfreezing the last layer of the pretrained CNN\nun_b5c1 = ['block5_conv1']\nUnfreeze_Layers(cnn_vgg, un_b5c1)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:38:21.298044Z","iopub.execute_input":"2022-02-08T03:38:21.298586Z","iopub.status.idle":"2022-02-08T03:38:21.310531Z","shell.execute_reply.started":"2022-02-08T03:38:21.298543Z","shell.execute_reply":"2022-02-08T03:38:21.309704Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"b5_c1.compile(loss='categorical_crossentropy',\n                optimizer='adam',\n                metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\n\nb5_c1_results = b5_c1.fit(x=train_img, y=train_lab,\n                                         batch_size=32,\n                                         steps_per_epoch=2700//32+1,# number of samples / batch size\n                                         epochs=25,\n                                        callbacks= b5c1_early,\n                                         validation_data=(test_img, test_lab),\n                                        validation_steps=394//32+1)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:38:25.753815Z","iopub.execute_input":"2022-02-08T03:38:25.754064Z","iopub.status.idle":"2022-02-08T03:39:42.263632Z","shell.execute_reply.started":"2022-02-08T03:38:25.754036Z","shell.execute_reply":"2022-02-08T03:39:42.262886Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"visualize_training_results(b5_c1_results)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:40:45.950945Z","iopub.execute_input":"2022-02-08T03:40:45.951221Z","iopub.status.idle":"2022-02-08T03:40:50.408704Z","shell.execute_reply.started":"2022-02-08T03:40:45.951192Z","shell.execute_reply":"2022-02-08T03:40:50.408061Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"b5_c1.evaluate(test_img, test_lab)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:40:57.421611Z","iopub.execute_input":"2022-02-08T03:40:57.422119Z","iopub.status.idle":"2022-02-08T03:40:58.271861Z","shell.execute_reply.started":"2022-02-08T03:40:57.422085Z","shell.execute_reply":"2022-02-08T03:40:58.271095Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"## **Adding Dropout layers to VGG-19 pretrained network (one layer unfrozen)**","metadata":{}},{"cell_type":"code","source":"# Making early stop for model\nvgg_drop_early = [EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True),\n            ModelCheckpoint(filepath='vgg_drop_model.h5', monitor='val_loss',\n                           save_best_only=True)]","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:43:51.667388Z","iopub.execute_input":"2022-02-08T03:43:51.670240Z","iopub.status.idle":"2022-02-08T03:43:51.674327Z","shell.execute_reply.started":"2022-02-08T03:43:51.670191Z","shell.execute_reply":"2022-02-08T03:43:51.673654Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Build first model using pretrained VGG 19 as first layer, and then some dense layers on top\nvgg_drop = keras.Sequential()\nvgg_drop.add(cnn_vgg)\nvgg_drop.add(layers.Flatten())\nvgg_drop.add(layers.Dropout(0.4))\nvgg_drop.add(layers.Dense(128, activation='relu'))\nvgg_drop.add(layers.Dropout(0.2))\nvgg_drop.add(layers.Dense(4, activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:43:55.117768Z","iopub.execute_input":"2022-02-08T03:43:55.118030Z","iopub.status.idle":"2022-02-08T03:43:55.200949Z","shell.execute_reply.started":"2022-02-08T03:43:55.117999Z","shell.execute_reply":"2022-02-08T03:43:55.200261Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Freeze all layers\nFreeze_Pretrained_Base(cnn_vgg, vgg_drop)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:44:05.259689Z","iopub.execute_input":"2022-02-08T03:44:05.260396Z","iopub.status.idle":"2022-02-08T03:44:05.267478Z","shell.execute_reply.started":"2022-02-08T03:44:05.260359Z","shell.execute_reply":"2022-02-08T03:44:05.266755Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Unfreezing the last layer of the pretrained CNN\nun_b5c1 = ['block5_conv1']\nUnfreeze_Layers(cnn_vgg, un_b5c1)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:44:12.741942Z","iopub.execute_input":"2022-02-08T03:44:12.742217Z","iopub.status.idle":"2022-02-08T03:44:12.754024Z","shell.execute_reply.started":"2022-02-08T03:44:12.742189Z","shell.execute_reply":"2022-02-08T03:44:12.753296Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"vgg_drop.compile(loss='categorical_crossentropy',\n                optimizer='adam',\n                metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\n\nvgg_drop_results = vgg_drop.fit(x=train_img, y=train_lab,\n                                            batch_size=32,\n                                         steps_per_epoch=2700//32+1,# number of samples / batch size\n                                         epochs=25,\n                                        callbacks= vgg_drop_early,\n                                         validation_data=(test_img, test_lab),\n                                           validation_steps=394//32+1)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:44:19.725247Z","iopub.execute_input":"2022-02-08T03:44:19.725515Z","iopub.status.idle":"2022-02-08T03:45:38.077620Z","shell.execute_reply.started":"2022-02-08T03:44:19.725487Z","shell.execute_reply":"2022-02-08T03:45:38.076884Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"visualize_training_results(vgg_drop_results)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:45:38.079120Z","iopub.execute_input":"2022-02-08T03:45:38.079379Z","iopub.status.idle":"2022-02-08T03:45:44.480218Z","shell.execute_reply.started":"2022-02-08T03:45:38.079343Z","shell.execute_reply":"2022-02-08T03:45:44.478591Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"vgg_drop.evaluate(test_img, test_lab)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:45:52.797684Z","iopub.execute_input":"2022-02-08T03:45:52.797943Z","iopub.status.idle":"2022-02-08T03:45:54.631599Z","shell.execute_reply.started":"2022-02-08T03:45:52.797915Z","shell.execute_reply":"2022-02-08T03:45:54.630934Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"## **Unfreezing another layer of VGG Pretrained Network**","metadata":{}},{"cell_type":"code","source":"# Making early stop for model\nb5_c1c2_early = [EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True),\n            ModelCheckpoint(filepath='b5c1c2_model.h5', monitor='val_acc',\n                           save_best_only=True)]","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:59:29.530862Z","iopub.execute_input":"2022-02-08T03:59:29.531164Z","iopub.status.idle":"2022-02-08T03:59:29.535213Z","shell.execute_reply.started":"2022-02-08T03:59:29.531133Z","shell.execute_reply":"2022-02-08T03:59:29.534553Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# Build first model using pretrained VGG 19 as first layer, and then some dense layers on top\nb5_c1c2 = keras.Sequential()\nb5_c1c2.add(cnn_vgg)\nb5_c1c2.add(layers.Flatten())\nb5_c1c2.add(layers.Dropout(0.4))\nb5_c1c2.add(layers.Dense(128, activation='relu'))\nb5_c1c2.add(layers.Dropout(0.2))\nb5_c1c2.add(layers.Dense(4, activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:52:36.004317Z","iopub.execute_input":"2022-02-08T03:52:36.004578Z","iopub.status.idle":"2022-02-08T03:52:36.090290Z","shell.execute_reply.started":"2022-02-08T03:52:36.004552Z","shell.execute_reply":"2022-02-08T03:52:36.089589Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# Freeze all layers\nFreeze_Pretrained_Base(cnn_vgg, b5_c1c2)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:52:49.544117Z","iopub.execute_input":"2022-02-08T03:52:49.544659Z","iopub.status.idle":"2022-02-08T03:52:49.553934Z","shell.execute_reply.started":"2022-02-08T03:52:49.544624Z","shell.execute_reply":"2022-02-08T03:52:49.552898Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# Unfreezing the last layer of the pretrained CNN\nun_b5c1c2 = ['block5_conv1', 'block5_conv2']\nUnfreeze_Layers(cnn_vgg, un_b5c1c2)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:52:58.465806Z","iopub.execute_input":"2022-02-08T03:52:58.466385Z","iopub.status.idle":"2022-02-08T03:52:58.477409Z","shell.execute_reply.started":"2022-02-08T03:52:58.466348Z","shell.execute_reply":"2022-02-08T03:52:58.476519Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"b5_c1c2.compile(loss='categorical_crossentropy',\n                optimizer='adam',\n                metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\n\nb5_c1c2_results = b5_c1c2.fit(x=train_img, y=train_lab,\n                                            batch_size=32,\n                                         steps_per_epoch=2700//32+1,# number of samples / batch size\n                                         epochs=25,\n                                        callbacks= vgg_drop_early,\n                                         validation_data=(test_img, test_lab))\n                                           #validation_steps=394//32+1)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T04:20:48.503762Z","iopub.execute_input":"2022-02-08T04:20:48.504031Z","iopub.status.idle":"2022-02-08T04:21:56.392484Z","shell.execute_reply.started":"2022-02-08T04:20:48.504001Z","shell.execute_reply":"2022-02-08T04:21:56.391728Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"visualize_training_results(b5_c1c2_results)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T04:22:05.209789Z","iopub.execute_input":"2022-02-08T04:22:05.210047Z","iopub.status.idle":"2022-02-08T04:22:13.215733Z","shell.execute_reply.started":"2022-02-08T04:22:05.210018Z","shell.execute_reply":"2022-02-08T04:22:13.215058Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"b5_c1c2.evaluate(test_img, test_lab)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T04:22:24.019989Z","iopub.execute_input":"2022-02-08T04:22:24.020261Z","iopub.status.idle":"2022-02-08T04:22:24.871769Z","shell.execute_reply.started":"2022-02-08T04:22:24.020213Z","shell.execute_reply":"2022-02-08T04:22:24.871066Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"## **Implementing Learning Rate Reduction**","metadata":{}},{"cell_type":"code","source":"# Making early stop for model\nred_c1c2_early = [EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),\n            ModelCheckpoint(filepath='b5c1c2_model.h5', monitor='val_loss', save_best_only=True),\n            ReduceLROnPlateau(patience=12, verbose=1)]","metadata":{"execution":{"iopub.status.busy":"2022-02-08T04:44:51.360656Z","iopub.execute_input":"2022-02-08T04:44:51.360910Z","iopub.status.idle":"2022-02-08T04:44:51.365704Z","shell.execute_reply.started":"2022-02-08T04:44:51.360881Z","shell.execute_reply":"2022-02-08T04:44:51.364677Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"# Build first model using pretrained VGG 19 as first layer, and then some dense layers on top\nred_c1c2 = keras.Sequential()\nred_c1c2.add(cnn_vgg)\nred_c1c2.add(layers.Flatten())\nred_c1c2.add(layers.Dropout(0.4))\nred_c1c2.add(layers.Dense(128, activation='relu'))\nred_c1c2.add(layers.Dropout(0.2))\nred_c1c2.add(layers.Dense(4, activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2022-02-08T04:44:53.712429Z","iopub.execute_input":"2022-02-08T04:44:53.713025Z","iopub.status.idle":"2022-02-08T04:44:53.798604Z","shell.execute_reply.started":"2022-02-08T04:44:53.712987Z","shell.execute_reply":"2022-02-08T04:44:53.797899Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"# Freeze all layers\nFreeze_Pretrained_Base(cnn_vgg, red_c1c2)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T04:44:56.601202Z","iopub.execute_input":"2022-02-08T04:44:56.601736Z","iopub.status.idle":"2022-02-08T04:44:56.608728Z","shell.execute_reply.started":"2022-02-08T04:44:56.601701Z","shell.execute_reply":"2022-02-08T04:44:56.607996Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"# Unfreezing the last layer of the pretrained CNN\nun_b5c1c2 = ['block5_conv1', 'block5_conv2']\nUnfreeze_Layers(cnn_vgg, un_b5c1c2)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T04:44:59.308729Z","iopub.execute_input":"2022-02-08T04:44:59.310399Z","iopub.status.idle":"2022-02-08T04:44:59.322160Z","shell.execute_reply.started":"2022-02-08T04:44:59.310353Z","shell.execute_reply":"2022-02-08T04:44:59.321286Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"red_c1c2.compile(loss='categorical_crossentropy',\n                optimizer='adam',\n                metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\n\nred_c1c2_results = red_c1c2.fit(x=train_img, y=train_lab,\n                                            batch_size=32,\n                                         steps_per_epoch=2700//32+1,# number of samples / batch size\n                                         epochs=100,\n                                        callbacks= red_c1c2_early,\n                                         validation_data=(test_img, test_lab),\n                                           validation_steps=394//32+1)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T04:45:02.325735Z","iopub.execute_input":"2022-02-08T04:45:02.326133Z","iopub.status.idle":"2022-02-08T04:47:34.067814Z","shell.execute_reply.started":"2022-02-08T04:45:02.326093Z","shell.execute_reply":"2022-02-08T04:47:34.067087Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"visualize_training_results(red_c1c2_results)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T04:47:47.022715Z","iopub.execute_input":"2022-02-08T04:47:47.022964Z","iopub.status.idle":"2022-02-08T04:48:27.158216Z","shell.execute_reply.started":"2022-02-08T04:47:47.022933Z","shell.execute_reply":"2022-02-08T04:48:27.157563Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"red_c1c2.evaluate(test_img, test_lab)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T04:48:33.139556Z","iopub.execute_input":"2022-02-08T04:48:33.140282Z","iopub.status.idle":"2022-02-08T04:48:33.960853Z","shell.execute_reply.started":"2022-02-08T04:48:33.140217Z","shell.execute_reply":"2022-02-08T04:48:33.960133Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":"**Best Iteration!**\\\nloss: 1.5687031745910645,\\\nacc: 0.7461928725242615,\\\nrecall: 0.7461928725242615,\\\nprecision: 0.7577319741249084,\\\ntp: 294.0,\\\ntn: 1088.0,\\\nfp: 94.0,\\\nfn: 100.0","metadata":{}},{"cell_type":"markdown","source":"## **Adding Batch Normalization to VGG19 Pretrained Neural Network**","metadata":{}},{"cell_type":"code","source":"# Making early stop for model\nvgg_batch_early = [EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True),\n            ModelCheckpoint(filepath='vgg_batch_model.h5', monitor='val_loss',\n                           save_best_only=True)]","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:15:32.425902Z","iopub.execute_input":"2022-02-07T21:15:32.426193Z","iopub.status.idle":"2022-02-07T21:15:32.43075Z","shell.execute_reply.started":"2022-02-07T21:15:32.426157Z","shell.execute_reply":"2022-02-07T21:15:32.429727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build first model using pretrained VGG 19 as first layer, and then some dense layers on top\nvgg_batch = keras.Sequential()\nvgg_batch.add(cnn_vgg)\nvgg_batch.add(layers.Flatten())\nvgg_batch.add(layers.BatchNormalization())\nvgg_batch.add(layers.Dropout(0.4))\nvgg_batch.add(layers.Dense(128, activation='relu'))\nvgg_batch.add(layers.BatchNormalization())\nvgg_batch.add(layers.Dropout(0.2))\nvgg_batch.add(layers.Dense(4, activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:15:34.41544Z","iopub.execute_input":"2022-02-07T21:15:34.41595Z","iopub.status.idle":"2022-02-07T21:15:34.523136Z","shell.execute_reply.started":"2022-02-07T21:15:34.415911Z","shell.execute_reply":"2022-02-07T21:15:34.522425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Re-freezing everything except for the last layer of the pretrained CNN\n# Code structure from https://github.com/learn-co-curriculum/dsc-using-pretrained-networks-codealong\nFreeze_Pretrained_Base(cnn_vgg, vgg_batch)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:15:42.599693Z","iopub.execute_input":"2022-02-07T21:15:42.599954Z","iopub.status.idle":"2022-02-07T21:15:42.60843Z","shell.execute_reply.started":"2022-02-07T21:15:42.599922Z","shell.execute_reply":"2022-02-07T21:15:42.607401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vgg_batch.compile(loss='categorical_crossentropy',\n                optimizer='adam',\n                metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\n\nvgg_batch_results = vgg_batch.fit(x=train_img, y=train_lab,\n                                         steps_per_epoch=2700//32+1,\n                                          batch_size=32,\n                                         epochs=25,\n                                        callbacks= vgg_batch_early,\n                                         validation_data=(test_img, test_lab),\n                                         validation_steps=394//32+1)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:18:17.730044Z","iopub.execute_input":"2022-02-07T21:18:17.730611Z","iopub.status.idle":"2022-02-07T21:18:54.81875Z","shell.execute_reply.started":"2022-02-07T21:18:17.730571Z","shell.execute_reply":"2022-02-07T21:18:54.817987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_training_results(vgg_batch_results)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:19:15.815238Z","iopub.execute_input":"2022-02-07T21:19:15.815705Z","iopub.status.idle":"2022-02-07T21:19:16.217249Z","shell.execute_reply.started":"2022-02-07T21:19:15.815665Z","shell.execute_reply":"2022-02-07T21:19:16.216576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vgg_batch.evaluate(test_img, test_lab)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T21:19:35.518183Z","iopub.execute_input":"2022-02-07T21:19:35.51889Z","iopub.status.idle":"2022-02-07T21:19:35.903742Z","shell.execute_reply.started":"2022-02-07T21:19:35.518838Z","shell.execute_reply":"2022-02-07T21:19:35.903039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Adding Regularization VGG19 Pretrained Network**","metadata":{}},{"cell_type":"code","source":"# Making early stop for model\nreg_vgg_early = [EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True),\n            ModelCheckpoint(filepath='vgg_batch_model.h5', monitor='val_loss',\n                           save_best_only=True)]","metadata":{"execution":{"iopub.status.busy":"2022-02-07T22:00:27.50418Z","iopub.execute_input":"2022-02-07T22:00:27.504562Z","iopub.status.idle":"2022-02-07T22:00:27.508628Z","shell.execute_reply.started":"2022-02-07T22:00:27.504514Z","shell.execute_reply":"2022-02-07T22:00:27.50796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build first model using pretrained VGG 19 as first layer, and then some dense layers on top\nreg_vgg = keras.Sequential()\nreg_vgg.add(cnn_vgg)\nreg_vgg.add(layers.Dropout(0.4))\nreg_vgg.add(layers.Flatten())\nreg_vgg.add(layers.Dense(128, activation='relu', kernel_regularizer=l2(l2=0.001)))\nreg_vgg.add(layers.Dropout(0.2))\nreg_vgg.add(layers.Dense(4, activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2022-02-07T22:00:32.020527Z","iopub.execute_input":"2022-02-07T22:00:32.020782Z","iopub.status.idle":"2022-02-07T22:00:32.103275Z","shell.execute_reply.started":"2022-02-07T22:00:32.020752Z","shell.execute_reply":"2022-02-07T22:00:32.102608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Re-freezing everything except for the last layer of the pretrained CNN\n# Code structure from https://github.com/learn-co-curriculum/dsc-using-pretrained-networks-codealong\nFreeze_Pretrained_Base(cnn_vgg, reg_vgg)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T22:00:50.445774Z","iopub.execute_input":"2022-02-07T22:00:50.446043Z","iopub.status.idle":"2022-02-07T22:00:50.453536Z","shell.execute_reply.started":"2022-02-07T22:00:50.446011Z","shell.execute_reply":"2022-02-07T22:00:50.452588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reg_vgg.compile(loss='categorical_crossentropy',\n                optimizer='adam',\n                metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\n\nreg_vgg_results = reg_vgg.fit(x=train_img, y=train_lab,\n                              batch_size=32,\n                              steps_per_epoch=2700//32+1,# number of samples / batch size\n                              epochs=25,\n                             callbacks= reg_vgg_early,\n                            validation_data= (test_img, test_lab),\n                            validation_steps = 394//32+1)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T22:04:00.402809Z","iopub.execute_input":"2022-02-07T22:04:00.40307Z","iopub.status.idle":"2022-02-07T22:05:02.595751Z","shell.execute_reply.started":"2022-02-07T22:04:00.40304Z","shell.execute_reply":"2022-02-07T22:05:02.594995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_training_results(reg_vgg_results)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T22:05:02.597248Z","iopub.execute_input":"2022-02-07T22:05:02.597531Z","iopub.status.idle":"2022-02-07T22:05:24.460733Z","shell.execute_reply.started":"2022-02-07T22:05:02.597494Z","shell.execute_reply":"2022-02-07T22:05:24.459969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reg_vgg.evaluate(test_img, test_lab)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T22:05:38.37489Z","iopub.execute_input":"2022-02-07T22:05:38.375177Z","iopub.status.idle":"2022-02-07T22:05:38.817703Z","shell.execute_reply.started":"2022-02-07T22:05:38.375122Z","shell.execute_reply":"2022-02-07T22:05:38.816992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Changing Optimizer to SGD**","metadata":{}},{"cell_type":"code","source":"sgd_mom = SGD(learning_rate = 0.001, momentum=0.9, nesterov=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T22:27:16.780805Z","iopub.execute_input":"2022-02-07T22:27:16.781074Z","iopub.status.idle":"2022-02-07T22:27:16.786163Z","shell.execute_reply.started":"2022-02-07T22:27:16.781042Z","shell.execute_reply":"2022-02-07T22:27:16.784961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making early stop for model\nsgd_vgg_early = [EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True),\n            ModelCheckpoint(filepath='vgg_sgd_model.h5', monitor='val_loss',\n                           save_best_only=True)]","metadata":{"execution":{"iopub.status.busy":"2022-02-07T22:28:23.42604Z","iopub.execute_input":"2022-02-07T22:28:23.426592Z","iopub.status.idle":"2022-02-07T22:28:23.432361Z","shell.execute_reply.started":"2022-02-07T22:28:23.426552Z","shell.execute_reply":"2022-02-07T22:28:23.431191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build first model using pretrained VGG 19 as first layer, and then some dense layers on top\nsgd_vgg = keras.Sequential()\nsgd_vgg.add(cnn_vgg)\nsgd_vgg.add(layers.Dropout(0.4))\nsgd_vgg.add(layers.Flatten())\nsgd_vgg.add(layers.Dense(128, activation='relu'))\nsgd_vgg.add(layers.Dropout(0.2))\nsgd_vgg.add(layers.Dense(4, activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2022-02-07T22:28:27.820965Z","iopub.execute_input":"2022-02-07T22:28:27.821669Z","iopub.status.idle":"2022-02-07T22:28:27.905551Z","shell.execute_reply.started":"2022-02-07T22:28:27.821628Z","shell.execute_reply":"2022-02-07T22:28:27.904819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Re-freezing everything except for the last layer of the pretrained CNN\n# Code structure from https://github.com/learn-co-curriculum/dsc-using-pretrained-networks-codealong\nFreeze_Pretrained_Base(cnn_vgg, sgd_vgg)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T22:28:30.364534Z","iopub.execute_input":"2022-02-07T22:28:30.366305Z","iopub.status.idle":"2022-02-07T22:28:30.37386Z","shell.execute_reply.started":"2022-02-07T22:28:30.366255Z","shell.execute_reply":"2022-02-07T22:28:30.373027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sgd_vgg.compile(loss='categorical_crossentropy',\n                optimizer= sgd_mom,\n                metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\n\nsgd_vgg_results = sgd_vgg.fit(x=train_img, y=train_lab,\n                              batch_size=32,\n                              steps_per_epoch=2700//32+1,# number of samples / batch size\n                              epochs=25,\n                             callbacks= sgd_vgg_early,\n                            validation_data= (test_img, test_lab),\n                            validation_steps = 394//32+1)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T22:29:42.888701Z","iopub.execute_input":"2022-02-07T22:29:42.888968Z","iopub.status.idle":"2022-02-07T22:30:18.611617Z","shell.execute_reply.started":"2022-02-07T22:29:42.888938Z","shell.execute_reply":"2022-02-07T22:30:18.610895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_training_results(sgd_vgg_results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sgd_vgg.evaluate(test_img, test_lab)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"loss: 1.1398 - acc: 0.4833 - recall: 0.2544 - precision: 0.5687 - true_positives: 687.0000 - true_negatives: 7579.0000 - false_positives: 521.0000 - false_negatives: 2013.0000 - val_loss: 1.4112 - val_acc: 0.3909 - val_recall: 0.1218 - val_precision: 0.5106 - val_true_positives: 48.0000 - val_true_negatives: 1136.0000 - val_false_positives: 46.0000 - val_false_negatives: 346.0000","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Using Pretrained Weights from VGG-16**","metadata":{}},{"cell_type":"code","source":"from keras.applications.vgg16 import VGG16\nvgg16 = VGG16(weights='imagenet',\n               include_top=False,\n               input_shape=(200,200,3))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vgg16.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build first model using pretrained VGG 19 as first layer, and then some dense layers on top\nvgg16_model = keras.Sequential()\nvgg16_model.add(vgg16)\nvgg16_model.add(layers.Flatten())\nvgg16_model.add(layers.Dense(128, activation='relu'))\nvgg16_model.add(layers.Dense(4, activation='softmax'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vgg16.trainable = False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check to see that the pretrained layer is not trainable but that all others are\nfor layer in vgg16_model.layers:\n    print(layer.name, layer.trainable)\n    \nprint(len(vgg16_model.trainable_weights))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vgg16_model.compile(optimizer='adam',\n                    loss='categorical_crossentropy',\n                    metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vgg16_model_results = vgg16_model.fit_generator(train_generator,\n                                         steps_per_epoch=2699/20,# number of samples / batch size\n                                         epochs=20,\n                                        callbacks= early_stop2,\n                                         validation_data=test_generator)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Analysis of Model**\n\nThe epoch with the lowest testing loss had a training accuracy of 86% and a testing accuracy of 68%, with a training loss of 37% and a testing loss of 110%. Testing recall is 65%. Testing loss is slightly lower than the previous model using VGG19, but accuracy and recall is significantly lower, so it looks like using the VGG network with more layers is better. However, it is still a good idea to continue to try different pretrained networks.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.applications.inception_v3 import InceptionV3\nincep = InceptionV3(weights='imagenet',\n               include_top=False,\n               input_shape=(100,100,3))\n","metadata":{"execution":{"iopub.status.busy":"2022-02-07T22:09:18.844639Z","iopub.execute_input":"2022-02-07T22:09:18.845251Z","iopub.status.idle":"2022-02-07T22:09:21.898358Z","shell.execute_reply.started":"2022-02-07T22:09:18.845196Z","shell.execute_reply":"2022-02-07T22:09:21.897588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"incep.summary()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-07T22:09:24.371139Z","iopub.execute_input":"2022-02-07T22:09:24.371443Z","iopub.status.idle":"2022-02-07T22:09:24.502837Z","shell.execute_reply.started":"2022-02-07T22:09:24.37141Z","shell.execute_reply":"2022-02-07T22:09:24.502177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making early stop for model\nincep_early = [EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True),\n            ModelCheckpoint(filepath='incep_model.h5', monitor='val_loss',\n                           save_best_only=True)]","metadata":{"execution":{"iopub.status.busy":"2022-02-07T22:11:34.015313Z","iopub.execute_input":"2022-02-07T22:11:34.015863Z","iopub.status.idle":"2022-02-07T22:11:34.022223Z","shell.execute_reply.started":"2022-02-07T22:11:34.015816Z","shell.execute_reply":"2022-02-07T22:11:34.021484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build first model using pretrained VGG 19 as first layer, and then some dense layers on top\nincep_model = keras.Sequential()\nincep_model.add(incep)\nincep_model.add(layers.Flatten())\nincep_model.add(layers.Dense(128, activation='relu'))\nincep_model.add(layers.Dense(4, activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2022-02-07T22:11:36.371596Z","iopub.execute_input":"2022-02-07T22:11:36.371873Z","iopub.status.idle":"2022-02-07T22:11:36.884596Z","shell.execute_reply.started":"2022-02-07T22:11:36.371842Z","shell.execute_reply":"2022-02-07T22:11:36.883551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Freeze_Pretrained_Base(incep, incep_model)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T22:12:04.725568Z","iopub.execute_input":"2022-02-07T22:12:04.726012Z","iopub.status.idle":"2022-02-07T22:12:04.74224Z","shell.execute_reply.started":"2022-02-07T22:12:04.725973Z","shell.execute_reply":"2022-02-07T22:12:04.741502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"incep_model.compile(optimizer='adam',\n                    loss='categorical_crossentropy',\n                    metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\n\nincep_model_results = incep_model.fit(x=train_img, y=train_lab,\n                                      batch_size=32,\n                                        steps_per_epoch=2700//32+1,# number of samples / batch size\n                                         epochs=25,\n                                        callbacks= incep_early,\n                                         validation_data=(test_img, test_lab),\n                                     validation_steps=394//32+1)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T22:13:41.762063Z","iopub.execute_input":"2022-02-07T22:13:41.762763Z","iopub.status.idle":"2022-02-07T22:14:29.301007Z","shell.execute_reply.started":"2022-02-07T22:13:41.762725Z","shell.execute_reply":"2022-02-07T22:14:29.300277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_training_results(incep_model_results)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T22:14:34.199548Z","iopub.execute_input":"2022-02-07T22:14:34.199802Z","iopub.status.idle":"2022-02-07T22:15:08.537682Z","shell.execute_reply.started":"2022-02-07T22:14:34.199773Z","shell.execute_reply":"2022-02-07T22:15:08.536963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Analysis of Model**\n\nThe epoch with the lowest testing loss has a training accuracy of 91% and a testing accuracy of 75%, with a training loss of 25% and a testing loss of 90%. Testing recall is around 73% This pretrained network resulted in the lowest testing loss yet, which is very promising; in the next few iterations I will try fine tuning this inception pretrained network.","metadata":{}},{"cell_type":"markdown","source":"## **Fine Tuning of Inception Network**\nAdding class weights to account for class imbalance, adding batch normalization and dropout layer, unfreezing outer most convolution layer of Inception Network","metadata":{}},{"cell_type":"code","source":"multi_weights = {0: 1,\n                1: 1 ,\n                2: 2,\n                3: 1,}\n# Add class weights\n# unfreeze outer layer\n# decrease learning rate","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build first model using pretrained VGG 19 as first layer, and then some dense layers on top\nincep2_model = keras.Sequential()\nincep2_model.add(incep)\nincep2_model.add(layers.Flatten())\nincep2_model.add(layers.Dense(512, activation='relu'))\nincep2_model.add(layers.BatchNormalization())\nincep2_model.add(layers.Dropout(0.3))\nincep2_model.add(layers.Dense(4, activation='softmax'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"incep.trainable = False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check to see that the pretrained layer is not trainable but that all others are\nfor layer in incep2_model.layers:\n    print(layer.name, layer.trainable)\n    \nprint(len(incep2_model.trainable_weights))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Re-freezing everything except for the last layer of the pretrained CNN\n# Code structure from https://github.com/learn-co-curriculum/dsc-using-pretrained-networks-codealong\nincep.trainable = True\nfor layer in  incep.layers:\n    if (layer.name == 'conv2d_93'):\n        layer.trainable = True\n    else:\n        layer.trainable = False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for layer in incep.layers:\n    print(layer.name, layer.trainable)\n    \nprint(len(incep.trainable_weights))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multi_weights = {0: 1,\n                1: 1 ,\n                2: 2,\n                3: 1,}\n\nincep2_model.compile(optimizer='adam',\n                    loss='categorical_crossentropy',\n                    metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\n\nincep2_model_results = incep2_model.fit_generator(train_generator,\n                                        class_weight= multi_weights,\n                                         steps_per_epoch=2699/20,# number of samples / batch size\n                                         epochs=20,\n                                        callbacks= early_stop2,\n                                         validation_data=test_generator)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_training_results(incep2_model_results)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Analysis of Model**\n\nThe epoch with the lowest testing loss had a training accuracy of 94% and a testing accuracy of 77%, with a training loss of 16% and a testing loss of 105%. Testing recall is 69%. Accuracy between this iteration and the last is similar, but testing loss is about ten percentage points higher, so the model still requires some tuning. ","metadata":{}},{"cell_type":"markdown","source":"## **Adjusting Previus Model by Removing Class Weights, Removing Batch Normalization, moving the Dropout Layer, and Unfreezing more Layers**","metadata":{}},{"cell_type":"code","source":"# Build first model using pretrained VGG 19 as first layer, and then some dense layers on top\nincep3_model = keras.Sequential()\nincep3_model.add(incep)\nincep3_model.add(layers.Dropout(0.5))\nincep3_model.add(layers.Flatten())\nincep3_model.add(layers.Dense(512, activation='relu'))\nincep3_model.add(layers.Dense(4, activation='softmax'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"incep.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unfreeze = ['conv2d_93', 'conv2d_85', 'conv2d_92', 'conv2d_91', 'conv2d_88', 'conv2d_87', 'conv2d_90', 'conv2d_86']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Unfreeze_Layers(incep, unfreeze)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"incep3_model.compile(optimizer='adam',\n                    loss='categorical_crossentropy',\n                    metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\n\nincep3_model_results = incep3_model.fit_generator(train_generator,\n                                         steps_per_epoch=2699/20,# number of samples / batch size\n                                         epochs=20,\n                                        callbacks= early_stop2,\n                                         validation_data=test_generator)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Analysis of Model**\n\nThe epoch with the lowest testing loss has a training accuracy of 83% and a testing accuracy of 66%, with a training loss of 45% and a testing loss of 103%. Testing recall is 62%. These numbers are definitely worse than other iterations, so it looks removing the class weights and batch normalization wasn't the best idea. Additionally, maybe too many layers have been unfrozen. Overall, it seems that too many things were changed at once.","metadata":{}},{"cell_type":"markdown","source":"## **Putting back Batch Normalization, Adding another Dropout Layer**","metadata":{}},{"cell_type":"code","source":"# Build first model using pretrained VGG 19 as first layer, and then some dense layers on top\nincep4_model = keras.Sequential()\nincep4_model.add(incep)\nincep4_model.add(layers.Dropout(0.4))\nincep4_model.add(layers.BatchNormalization())\nincep4_model.add(layers.Flatten())\nincep4_model.add(layers.Dense(512, activation='relu'))\nincep4_model.add(layers.Dropout(0.3))\nincep4_model.add(layers.BatchNormalization())\nincep4_model.add(layers.Dense(4, activation='softmax'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Unfreeze_Layers(incep, unfreeze)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multi_weights = {0: 1,\n                1: 1 ,\n                2: 2,\n                3: 1,}\n\nincep4_model.compile(optimizer='adam',\n                    loss='categorical_crossentropy',\n                    metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\n\nincep4_model_results = incep4_model.fit_generator(train_generator,\n                                           class_weight = multi_weights,       \n                                         steps_per_epoch=2699/20,# number of samples / batch size\n                                         epochs=20,\n                                        callbacks= early_stop2,\n                                         validation_data=test_generator)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Model with Inception Pretrained Network using a smaller learning rate**","metadata":{}},{"cell_type":"code","source":"# Build first model using pretrained VGG 19 as first layer, and then some dense layers on top\nincep3_model = keras.Sequential()\nincep3_model.add(incep)\nincep3_model.add(layers.Flatten())\nincep3_model.add(layers.Dense(512, activation='relu'))\nincep3_model.add(layers.BatchNormalization())\nincep3_model.add(layers.Dropout(0.3))\nincep3_model.add(layers.Dense(4, activation='softmax'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"incep.trainable = False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check to see that the pretrained layer is not trainable but that all others are\nfor layer in incep3_model.layers:\n    print(layer.name, layer.trainable)\n    \nprint(len(incep3_model.trainable_weights))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Re-freezing everything except for the last layer of the pretrained CNN\n# Code structure from https://github.com/learn-co-curriculum/dsc-using-pretrained-networks-codealong\nincep.trainable = True\nfor layer in  incep.layers:\n    if (layer.name == 'conv2d_93'):\n        layer.trainable = True\n    else:\n        layer.trainable = False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for layer in incep.layers:\n    print(layer.name, layer.trainable)\n    \nprint(len(incep.trainable_weights))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multi_weights = {0: 1,\n                1: 1 ,\n                2: 2,\n                3: 1,}\n\nsmall_lr_adam = keras.optimizers.Adam(learning_rate=0.0001)\n\nincep3_model.compile(optimizer= small_lr_adam,\n                    loss='categorical_crossentropy',\n                    metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\n\nincep3_model_results = incep3_model.fit_generator(train_generator,\n                                        class_weight= multi_weights,\n                                         steps_per_epoch=2699/20,# number of samples / batch size\n                                         epochs=20,\n                                        callbacks= early_stop2,\n                                         validation_data=test_generator)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_training_results(incep3_model_results)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Analysis of Model**\n\nThe epoch with the lowest testing loss had a training accuracy of 94% and a testing acuracy of 76%, with a training loss of 17% and a testing loss of around 99%; the model is still overfitting. Tetsing recall is 71%. Other than the testing loss being slightly lower in this iteration than the last, the numbers between this iteration and the ast are fairly similar; it is possible this model could improve with more training epochs. It is also possible that unfreezing more layers from the pretrained base might help the network better learn and adjust to these specific images.","metadata":{}},{"cell_type":"markdown","source":"## **Unfreezing more layers of the Pretrained InceptionV3 Network**","metadata":{}},{"cell_type":"code","source":"# Build first model using pretrained VGG 19 as first layer, and then some dense layers on top\nincep4_model = keras.Sequential()\nincep4_model.add(incep)\nincep4_model.add(layers.Flatten())\nincep4_model.add(layers.Dense(512, activation='relu'))\nincep4_model.add(layers.BatchNormalization())\nincep4_model.add(layers.Dropout(0.3))\nincep4_model.add(layers.Dense(4, activation='softmax'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"incep.trainable = False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check to see that the pretrained layer is not trainable but that all others are\nfor layer in incep4_model.layers:\n    print(layer.name, layer.trainable)\n    \nprint(len(incep4_model.trainable_weights))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Re-freezing everything except for the last layer of the pretrained CNN\n# Code structure from https://github.com/learn-co-curriculum/dsc-using-pretrained-networks-codealong\nincep.trainable = True\nfor layer in  incep.layers:\n    if (layer.name == 'conv2d_93') |  (layer.name == 'conv2d_85') \\\n    | (layer.name == 'conv2d_92') | (layer.name == 'conv2d_91'):\n        layer.trainable = True\n    else:\n        layer.trainable = False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"incep.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for layer in incep.layers:\n    print(layer.name, layer.trainable)\n    \nprint(len(incep.trainable_weights))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multi_weights = {0: 1,\n                1: 1 ,\n                2: 2,\n                3: 1,}\n\nsmall_lr_adam = keras.optimizers.Adam(learning_rate=0.0001)\n\nincep4_model.compile(optimizer= small_lr_adam,\n                    loss='categorical_crossentropy',\n                    metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\n\nincep4_model_results = incep4_model.fit_generator(train_generator,\n                                        class_weight= multi_weights,\n                                         steps_per_epoch=2699/20,# number of samples / batch size\n                                         epochs=20,\n                                        callbacks= early_stop2,\n                                         validation_data=test_generator)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Analysis of Model**\n\nThe epoch with the lowest testing loss ad a training ccuracy of 95% and a testing accuracy of 75%, with a training loss of 16% and a testing loss of 108%. Testing recall is 75%. the testing loss is slightly higher than the last model iteration, but other than that the results are pretty similar. It looks like unfreezing a few more layers did not help much; it could be that unfreezing more layers wil not help, but it is also possible that since there are so many layers in this pretrained network, that a lot of layers need to be unfrozen. In the next model iteration I will try unfreezing a few more layers and changing the opimizer to SGD with momentum, since this is an optimizer well known to get good results.","metadata":{}},{"cell_type":"markdown","source":"## **Using SGD with momentum as an Optimizer on pretrained InceptionV3 Network**","metadata":{}},{"cell_type":"code","source":"# Build first model using pretrained VGG 19 as first layer, and then some dense layers on top\nincep5_model = keras.Sequential()\nincep5_model.add(incep)\nincep5_model.add(layers.Flatten())\nincep5_model.add(layers.Dense(512, activation='relu'))\nincep5_model.add(layers.BatchNormalization())\nincep5_model.add(layers.Dropout(0.3))\nincep5_model.add(layers.Dense(4, activation='softmax'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"incep.trainable = False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check to see that the pretrained layer is not trainable but that all others are\nfor layer in incep5_model.layers:\n    print(layer.name, layer.trainable)\n    \nprint(len(incep5_model.trainable_weights))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Re-freezing everything except for the last layer of the pretrained CNN\n# Code structure from https://github.com/learn-co-curriculum/dsc-using-pretrained-networks-codealong\nincep.trainable = True\nfor layer in  incep.layers:\n    if (layer.name == 'conv2d_93') |  (layer.name == 'conv2d_85') \\\n    | (layer.name == 'conv2d_92') | (layer.name == 'conv2d_91'):\n        layer.trainable = True\n    else:\n        layer.trainable = False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for layer in incep.layers:\n    print(layer.name, layer.trainable)\n    \nprint(len(incep.trainable_weights))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multi_weights = {0: 1,\n                1: 1 ,\n                2: 2,\n                3: 1,}\n\nsgd_momen = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n\nincep5_model.compile(optimizer= sgd_momen,\n                    loss='categorical_crossentropy',\n                    metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\n\nincep5_model_results = incep5_model.fit_generator(train_generator,\n                                        class_weight= multi_weights,\n                                         steps_per_epoch=2699/20,# number of samples / batch size\n                                         epochs=20,\n                                        callbacks= early_stop2,\n                                         validation_data=test_generator)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_training_results(incep5_model_results)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Analysis of Model**\n\nThe epoch with the lowest loss had a training accuracy of 95% and a testing acuracy of 76%, with a training loss of 15% and a testing loss of 105%. Testing recall is 75%. The results from this model iteration are not much different from the last, meaning that using the SGD optimizer with momentum did not help in this instance. Perhaps adding a dropout layer just after the pretrained network base will improve results.","metadata":{}},{"cell_type":"markdown","source":"## **Adding another dropout layer to Model Using Pretrained InceptionV3 Network**","metadata":{}},{"cell_type":"code","source":"# Build first model using pretrained VGG 19 as first layer, and then some dense layers on top\nincep6_model = keras.Sequential()\nincep6_model.add(incep)\nincep6_model.add(layers.Dropout(0.5))\nincep6_model.add(layers.Flatten())\nincep6_model.add(layers.Dense(512, activation='relu'))\nincep6_model.add(layers.BatchNormalization())\nincep6_model.add(layers.Dropout(0.3))\nincep6_model.add(layers.Dense(4, activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2022-02-06T17:14:56.737383Z","iopub.execute_input":"2022-02-06T17:14:56.737927Z","iopub.status.idle":"2022-02-06T17:14:57.279509Z","shell.execute_reply.started":"2022-02-06T17:14:56.737887Z","shell.execute_reply":"2022-02-06T17:14:57.278784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"incep.trainable = False","metadata":{"execution":{"iopub.status.busy":"2022-02-06T17:15:00.476882Z","iopub.execute_input":"2022-02-06T17:15:00.477258Z","iopub.status.idle":"2022-02-06T17:15:00.49072Z","shell.execute_reply.started":"2022-02-06T17:15:00.477219Z","shell.execute_reply":"2022-02-06T17:15:00.489845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check to see that the pretrained layer is not trainable but that all others are\nfor layer in incep6_model.layers:\n    print(layer.name, layer.trainable)\n    \nprint(len(incep6_model.trainable_weights))","metadata":{"execution":{"iopub.status.busy":"2022-02-06T17:15:46.730049Z","iopub.execute_input":"2022-02-06T17:15:46.730769Z","iopub.status.idle":"2022-02-06T17:15:46.738553Z","shell.execute_reply.started":"2022-02-06T17:15:46.73073Z","shell.execute_reply":"2022-02-06T17:15:46.737837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"incep.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Re-freezing everything except for the last layer of the pretrained CNN\n# Code structure from https://github.com/learn-co-curriculum/dsc-using-pretrained-networks-codealong\nincep.trainable = True\nfor layer in  incep.layers:\n    if (layer.name == 'conv2d_93') |  (layer.name == 'conv2d_85') \\\n    | (layer.name == 'conv2d_92') | (layer.name == 'conv2d_91') | \\\n      (layer.name == 'conv2d_88') | (layer.name == 'conv2d_87') | \\\n      (layer.name == 'conv2d_90') | (layer.name == 'conv2d_86'):\n        layer.trainable = True\n    else:\n        layer.trainable = False","metadata":{"execution":{"iopub.status.busy":"2022-02-06T17:15:52.465368Z","iopub.execute_input":"2022-02-06T17:15:52.465882Z","iopub.status.idle":"2022-02-06T17:15:52.493213Z","shell.execute_reply.started":"2022-02-06T17:15:52.465845Z","shell.execute_reply":"2022-02-06T17:15:52.492478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for layer in incep.layers:\n    print(layer.name, layer.trainable)\n    \nprint(len(incep.trainable_weights))","metadata":{"execution":{"iopub.status.busy":"2022-02-06T17:15:56.238571Z","iopub.execute_input":"2022-02-06T17:15:56.239403Z","iopub.status.idle":"2022-02-06T17:15:56.333451Z","shell.execute_reply.started":"2022-02-06T17:15:56.239354Z","shell.execute_reply":"2022-02-06T17:15:56.332788Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making early stop for model\nearly_stop2 = [EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True),\n            ModelCheckpoint(filepath='best_model.h5', monitor='val_loss',\n                           save_best_only=True)]","metadata":{"execution":{"iopub.status.busy":"2022-02-06T17:19:13.86171Z","iopub.execute_input":"2022-02-06T17:19:13.861977Z","iopub.status.idle":"2022-02-06T17:19:13.866767Z","shell.execute_reply.started":"2022-02-06T17:19:13.861946Z","shell.execute_reply":"2022-02-06T17:19:13.86558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multi_weights = {0: 1,\n                1: 1 ,\n                2: 2,\n                3: 1,}\n\nsgd_momen = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n\nincep6_model.compile(optimizer= sgd_momen,\n                    loss='categorical_crossentropy',\n                    metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\n\nincep6_model_results = incep6_model.fit(x=train_img, y=train_lab,\n                                        batch_size=32,\n                                        class_weight= multi_weights,\n                                         steps_per_epoch=2700//32+1,# number of samples / batch size\n                                         epochs=25,\n                                        callbacks= early_stop2,\n                                         validation_data=(test_img, test_lab),\n                                        validation_steps= 394//32+1)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T17:23:21.547497Z","iopub.execute_input":"2022-02-06T17:23:21.547767Z","iopub.status.idle":"2022-02-06T17:24:23.50734Z","shell.execute_reply.started":"2022-02-06T17:23:21.547738Z","shell.execute_reply":"2022-02-06T17:24:23.506416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Analysis of Model**\n\nThe eposhc with the lowest testing loss had a training accuracy of 95% and a testing accuracy of 78%, with a training loss of 15% and a testing loss of 107%. Testing recall is 78%.","metadata":{}},{"cell_type":"markdown","source":"## **Trying out EfficientNetB4 Pretrained Network**","metadata":{}},{"cell_type":"code","source":"from keras.applications.efficientnet import EfficientNetB4\neff_b4 = EfficientNetB4(weights='imagenet',\n               include_top=False,\n               input_shape=(200,200,3))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build first model using pretrained VGG 19 as first layer, and then some dense layers on top\neffb4_model = keras.Sequential()\neffb4_model.add(eff_b4)\neffb4_model.add(layers.Flatten())\neffb4_model.add(layers.Dense(500, activation='relu'))\neffb4_model.add(layers.Dense(4, activation='softmax'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Freeze_Pretrained_Base(eff_b4, effb4_model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multi_weights = {0: 1,\n                1: 1 ,\n                2: 2,\n                3: 1,}\n\nsmall_adam = keras.optimizers.Adam(learning_rate=0.0001)\neffb4_model.compile(optimizer= small_adam,\n                    loss='categorical_crossentropy',\n                    metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\n\neffb4_model_results = effb4_model.fit_generator(train_generator,\n                                        class_weight= multi_weights,\n                                         steps_per_epoch=2699/20,# number of samples / batch size\n                                         epochs=30,\n                                        callbacks= early_stop2,\n                                         validation_data=test_generator)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Analysis of Model** \n\nThe epoch with the lowest testing loss had a training accuracy of 27% and a testing accuracy of 27%, with a training loss of 203% and a testing loss of 140%. Testing recall is 0%. It is air to say that this model performed abysmally! ","metadata":{}},{"cell_type":"code","source":"from keras.applications.efficientnet import EfficientNetB1\neff_b1 = EfficientNetB1(weights='imagenet',\n               include_top=False,\n               input_shape=(200,200,3))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build first model using pretrained VGG 19 as first layer, and then some dense layers on top\ntwo_effb1 = keras.Sequential()\ntwo_effb1.add(eff_b1)\ntwo_effb1.add(layers.GlobalAveragePooling2D())\ntwo_effb1.add(layers.Dropout(0.45))\ntwo_effb1.add(layers.Dense(4, activation='softmax'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reduce_lr = keras.callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\",\n    factor=0.2,\n    patience=2,\n    verbose=1,\n    mode=\"auto\",\n    min_delta=0.0001,\n    cooldown=0,\n    min_lr=0.00001)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Freeze_Pretrained_Base(eff_b1, two_effb1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multi_weights = {0: 2,\n                1: 1 ,\n                2: 1,\n                3: 1,}\n\n#sgd_momen = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\ntwo_effb1.compile(optimizer= 'adam',\n                    loss='categorical_crossentropy',\n                    metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\n\ntwo_effb1_results = two_effb1.fit_generator(train_generator,\n                                        class_weight= multi_weights,\n                                         steps_per_epoch=2699// 20+1,# number of samples / batch size\n                                         epochs=20,\n                                        callbacks= [early_stop2, reduce_lr],\n                                         validation_data=test_generator,\n                                           validation_steps=394//20+1)","metadata":{},"execution_count":null,"outputs":[]}]}