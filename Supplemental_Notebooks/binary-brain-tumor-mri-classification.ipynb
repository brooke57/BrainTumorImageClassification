{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **Important Note**\n\n## I began doing the project in this notebook, however I moved to Google Colab for more GPU time, so this notebook is incomplete. Please see the notebook \"Final Binary Brain Tumor Classification,\" for the relevant models or the notebook called \"All Models for Binary Brain Tumor Classification\" for all model iterations.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"execution":{"iopub.status.busy":"2021-12-02T05:03:22.364596Z","iopub.execute_input":"2021-12-02T05:03:22.365151Z","iopub.status.idle":"2021-12-02T05:03:23.121821Z","shell.execute_reply.started":"2021-12-02T05:03:22.365055Z","shell.execute_reply":"2021-12-02T05:03:23.121105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set random state for numpy operations\nfrom numpy.random import seed\nseed(2)\n# Set random state for tensorflow operations\nfrom tensorflow.random import set_seed\nset_seed(3)\n# General imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import regularizers\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.models import load_model\nimport seaborn as sns\nfrom mlxtend.plotting import plot_decision_regions\nfrom sklearn.metrics import confusion_matrix\nimport cv2\nimport PIL","metadata":{"execution":{"iopub.status.busy":"2021-12-02T05:03:26.932556Z","iopub.execute_input":"2021-12-02T05:03:26.932857Z","iopub.status.idle":"2021-12-02T05:03:28.954948Z","shell.execute_reply.started":"2021-12-02T05:03:26.932824Z","shell.execute_reply":"2021-12-02T05:03:28.954196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Business Understanding**\n\nBrain tumors in particular are very difficult to diagnose from an MRI image, and artificial intelligence methods of identifying and classifying tumors are oftentimes more accurate than manual identification by a radiologist. That is why the development of neural networks and other AI processes for tumor classification is so valuable and important.\n\nThe survival rate for patients diagnosed with a brain tumor is around 35%. This survival rate could be increased if tumors could be identified earlier and more accurately, which AI methods could help with. Additionally, in third world countries, seasoned neurosurgeons (a neurosurgeon is required to make the diagnoses from looking at the MRI) are hard to come by, so a machine learning tool (Decision Support Tool) which could accurately identify tumors would be of great value in these developing nations. This decision support tool would be beneficial to the health industry, and the target audience would be Doctors without Borders, an organization which sends doctors from the US to developing countries to help improve their healthcare.","metadata":{}},{"cell_type":"code","source":"\ndef visualize_training_results(history):\n    '''\n    From https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n    \n    Input: keras history object (output from trained model)\n    '''\n    fig, (ax1, ax2) = plt.subplots(2, sharex=True)\n    fig.suptitle('Model Results')\n\n    # summarize history for accuracy\n    ax1.plot(history.history['acc'])\n    ax1.plot(history.history['val_acc'])\n    ax1.set_ylabel('Accuracy')\n    ax1.legend(['train', 'test'], loc='upper left')\n    # summarize history for loss\n    ax2.plot(history.history['loss'])\n    ax2.plot(history.history['val_loss'])\n    ax2.set_ylabel('Loss')\n    ax2.legend(['train', 'test'], loc='upper left')\n    \n    plt.xlabel('Epoch')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-02T05:03:36.127042Z","iopub.execute_input":"2021-12-02T05:03:36.127322Z","iopub.status.idle":"2021-12-02T05:03:36.134484Z","shell.execute_reply.started":"2021-12-02T05:03:36.12729Z","shell.execute_reply":"2021-12-02T05:03:36.133449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set up ImageDataGenerator\ntrain_imagegen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255,\n                                   zoom_range=([0.6,1]),\n                                   rotation_range=10,                           \n                                   brightness_range=([0.6, 1.5]),\n                                   horizontal_flip=True,\n                                   validation_split=0.06) # this will set aside a part of training set for validation data\ntest_imagegen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255,\n                                   zoom_range=([0.6,1]),\n                                   rotation_range=10,\n                                   brightness_range=([0.6,1.5]),\n                                   horizontal_flip=True)\n# Bring the data in\ntrain_generator = train_imagegen.flow_from_directory(\n                                    '../input/resortedbraintumorclassificationmridata/Brain_MRI_Tumor_Images/Training',\n                                    classes={'no_tumor_train':0,\n                                            'AllTumorsTrain':1},\n                                    target_size=(200,200),\n                                    batch_size=20,\n                                    seed=42,\n                                    class_mode='binary',\n                                    subset='training')\n\ntest_generator = test_imagegen.flow_from_directory(\n                                    '../input/resortedbraintumorclassificationmridata/Brain_MRI_Tumor_Images/Testing',\n                                     classes={'no_tumor_test':0,\n                                            'AllTumorsTest':1},\n                                    target_size=(200,200),\n                                    batch_size=20,\n                                    seed=42,\n                                    class_mode='binary')\n\nval_generator = train_imagegen.flow_from_directory(\n                                    '../input/resortedbraintumorclassificationmridata/Brain_MRI_Tumor_Images/Training',\n                                     classes={'no_tumor_train':0,\n                                            'AllTumorsTrain':1},\n                                    target_size=(200,200),\n                                    batch_size=20,\n                                    seed=42,\n                                    class_mode='binary',\n                                    subset='validation')","metadata":{"execution":{"iopub.status.busy":"2021-12-02T05:03:56.669335Z","iopub.execute_input":"2021-12-02T05:03:56.669801Z","iopub.status.idle":"2021-12-02T05:03:57.096284Z","shell.execute_reply.started":"2021-12-02T05:03:56.669762Z","shell.execute_reply":"2021-12-02T05:03:57.095482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Important Note**\n\nOne important thing to note is that throughout the modelling process, I use train_generator as my training data images, and testing_generator as my testing set which I use every time I run a model to asses overfitting. Val_generator data is used only once on the final model, to asses how well the model does on data is has never seen before. This clarification is important because for every model that is run, all of the epochs are printed out, and the metrics/results for testing data are referred to as 'val.' Unless otherwise specified, any metric starting with 'val' in the epich print outs is really testing data.","metadata":{}},{"cell_type":"markdown","source":"# **Data Understanding**\n\nThis data is composed of a series of Brain MRIs consisting of scans which contain a tumor and those that do not. The data actually comes from an existing kaggle dataset ([\"Brain Tumor Classification (MRI)\"](https://www.kaggle.com/sartajbhuvaji/brain-tumor-classification-mri)), which further divided the tumor data into three different types: Glioma, Meningioma, and Pituitary. I downloaded this data and combined all the tumor scans into one category and then uploaded it onto kaggle, so that I could make this a binary classification question. There are a variety of different planes, or perspectives, from which the scans are taken; some are sagital scans (plane that shows the side of the brain), some are coronal scans (plane that shows the back of the brain, at varying depths), and some are transverse (plane that shows the top of the brain, at varying depths; like a bird's eye view). The dataset contains MRIs taken with a variety of different methods, namely T1, T2, and FLAIR. Each of these different methods results in an MRI image with varying levels of brightness and contrast. The data contains 2,764 tumor MRIs and 500 normal MRIs, so the dataset is very imbalanced. Each image uses all three color channels, and each is a different size, so I standardized all images to be 200 x 200 x 3.","metadata":{}},{"cell_type":"markdown","source":"## **Data Augmentation**\n\nBecause a total of 2,870 files (number of files in the training set) is a pretty small number of images to use for training a neural network, data augmentation is key, since it can help mimic the effect of having a larger number of images. The way I implemented these techiques was by using the ImageDataGenerator from Keras. The features I decided to tweak for augmentation were zoom range, rotation range, brightness range, and horizontal flipping. I decided to provided a range of different zoom values and rotation degrees because how much zoom and the angle of how the brain is positioned in an MRI image can vary a little, and sp producing images with varying levels of zoom and rotation is a realistic way to mimic the effect of having more images. I decided to provide a range for brightness level because as mentioned in the 'Data Understanding' paragraph above, the dataset contains a variety of images with different levels of brightness and contrast, and so producing images with different levels of brightness is a realistic way to mimic the effect of having more images. I decided to flip some images along the horizontal axis, which translates to a left right flip, because regions of the brain are very symmetrical along the left/right axis. I did not include vertical flipping as part of data augmentation, because top/bottom parts of the brain are not symmetrical. I also decided not to shear any images, because shearing stretches and distorts regions of an image, and for brain scans it is very important to preserve the correct anatomical structure of the brain, as discussed in this [this dataset](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6917660/).","metadata":{}},{"cell_type":"markdown","source":"## **Taking a Look at a few different individual images**","metadata":{}},{"cell_type":"code","source":"tumor1 = PIL.Image.open('../input/resortedbraintumorclassificationmridata/Brain_MRI_Tumor_Images/Training/AllTumorsTrain/p (274).jpg')\ntumor1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Figuring out the number of color channels\ntumor1.mode\n# It is 'RGB', meaning that there are three color channels 471, 490","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images = ['../input/resortedbraintumorclassificationmridata/Brain_MRI_Tumor_Images/Training/no_tumor_train/image (11).jpg',\n         '../input/resortedbraintumorclassificationmridata/Brain_MRI_Tumor_Images/Training/no_tumor_train/image(115).jpg',\n         '../input/resortedbraintumorclassificationmridata/Brain_MRI_Tumor_Images/Training/no_tumor_train/image(108).jpg',\n         '../input/resortedbraintumorclassificationmridata/Brain_MRI_Tumor_Images/Training/no_tumor_train/image(100).jpg',\n         '../input/resortedbraintumorclassificationmridata/Brain_MRI_Tumor_Images/Training/no_tumor_train/image(132).jpg',\n         '../input/resortedbraintumorclassificationmridata/Brain_MRI_Tumor_Images/Training/no_tumor_train/image(145).jpg',\n         '../input/resortedbraintumorclassificationmridata/Brain_MRI_Tumor_Images/Training/AllTumorsTrain/gg (105).jpg',\n         '../input/resortedbraintumorclassificationmridata/Brain_MRI_Tumor_Images/Training/AllTumorsTrain/gg (12).jpg',\n         '../input/resortedbraintumorclassificationmridata/Brain_MRI_Tumor_Images/Training/AllTumorsTrain/m (126).jpg',\n         '../input/resortedbraintumorclassificationmridata/Brain_MRI_Tumor_Images/Training/AllTumorsTrain/m (130).jpg',\n         '../input/resortedbraintumorclassificationmridata/Brain_MRI_Tumor_Images/Training/AllTumorsTrain/p (471).jpg',\n         '../input/resortedbraintumorclassificationmridata/Brain_MRI_Tumor_Images/Training/AllTumorsTrain/p (490).jpg']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from https://stackoverflow.com/questions/48435229/how-to-plot-a-list-of-image-in-loop-using-matplotlib/48435411\nfig, axes = plt.subplots(2, 6, figsize=(18, 10))\nrows = 2\n\nfor num, x in enumerate(images):\n    img = PIL.Image.open(x)\n    re_img = img.resize((200,200))\n    ax = axes[num // 6, num % 6]\n    ax.imshow(re_img)\n    \n\nfig.suptitle('Normal Images on Top Row, Tumor Images on Bottom Row', fontsize=20)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Taking a look at the class imbalance**","metadata":{}},{"cell_type":"markdown","source":"### **First looking at training data imbalance**","metadata":{}},{"cell_type":"code","source":"# looking at how the categories are encoded\ntrain_generator.class_indices\n# no tuumor data is encoded as zero, tumor data is encoded as one","metadata":{"execution":{"iopub.status.busy":"2021-12-02T05:04:22.834075Z","iopub.execute_input":"2021-12-02T05:04:22.834692Z","iopub.status.idle":"2021-12-02T05:04:22.843538Z","shell.execute_reply.started":"2021-12-02T05:04:22.834653Z","shell.execute_reply":"2021-12-02T05:04:22.841684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Looking at the place where all labels for training data are stored \ntrain_generator.classes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making a DataFrame out of the training data labels\ntrain_tumors = pd.DataFrame(train_generator.classes)\ntrain_values = train_tumors.value_counts()\ntrain_values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **The ratio of images with tumors to those without is 2327:372, or 6.255:1.**","metadata":{}},{"cell_type":"code","source":"# Making subsets of the dataframe for visualization purposes\ntrain_tumors.rename(columns={0:'Tumor/No Tumor'}, inplace=True)\ntrain_no_tumor = len(train_tumors[train_tumors['Tumor/No Tumor'] == 0])\ntrain_tumor = len(train_tumors[train_tumors['Tumor/No Tumor'] == 1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Barplot for visually assessing the training data class imbalance\nplt.figure(figsize=(10,8))\nsns.set(font_scale=1.4)\nsns.barplot(['No Tumor', 'Tumor'], [train_no_tumor, train_tumor])\nplt.ylabel(\"Number of Images\")\nplt.title('Distribution of Brain MRIs with and without Tumor');\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Now taking a look at the test data imbalance**","metadata":{}},{"cell_type":"code","source":"# Making a DataFrame of testing data labels\ntest_tumors = pd.DataFrame(test_generator.classes)\ntest_values = test_tumors.value_counts()\ntest_values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **The ratio of images with tumors to those without is 289:105, or 2.75:1**","metadata":{}},{"cell_type":"code","source":"# Making subsets of the dataframe for plotting purposes\ntest_tumors.rename(columns={0:'Tumor/No Tumor'}, inplace=True)\ntest_no_tumor = len(test_tumors[test_tumors['Tumor/No Tumor'] == 0])\ntest_tumor = len(test_tumors[test_tumors['Tumor/No Tumor'] == 1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Barplot for visually assessing the class imbalance in the testing data\nplt.figure(figsize=(10,8))\nsns.set(font_scale=1.4)\nsns.barplot(['No Tumor', 'Tumor'], [test_no_tumor, test_tumor])\nplt.ylabel(\"Number of Images\")\nplt.title('Distribution of Brain MRIs with and without Tumor in Testing Data');\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Baseline CNN Model**","metadata":{}},{"cell_type":"code","source":"# Building the first baseline model; structure is modified from one shown on:\n# https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/\nbaseline = keras.Sequential()\nbaseline.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(200,200,3)))\nbaseline.add(layers.MaxPooling2D(2,2))\nbaseline.add(layers.Conv2D(64, (3,3), activation='relu'))\nbaseline.add(layers.MaxPooling2D(2,2))\n\nbaseline.add(layers.Flatten())\nbaseline.add(layers.Dense(128, activation='relu'))\nbaseline.add(layers.Dense(1, activation='sigmoid'))\n\nbaseline.compile(loss='binary_crossentropy',\n                optimizer='adam',\n                metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Has been rerun\n# Info for how to construct steps_per_epoch value from:\n# https://stackoverflow.com/questions/46010565/checking-validation-results-in-keras-shows-only-50-correct-clearly-random\nbaseline_results = baseline.fit_generator(train_generator,\n                                         steps_per_epoch=2699 // 20 +1,# number of samples / batch size plus one, so that every batch is included\n                                         epochs=10,\n                                        validation_data=test_generator,\n                                         validation_steps= 394 // 20+1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_training_results(baseline_results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Analysis of Model**\n \nIn the last epoch, the training accuracy is 93% and testing accuracy is 58%, with a training loss of 17% and a testing loss of 76%. Testing recall is 50% and testing precision is 86%. Obviously, the model is overfitting, and loss is quite high; the recall score is very low as well. In the next model iteration, I will add another dense layer, which will hopefully help the model pick up on more patterns, and some dropout layers for a form of regularization.","metadata":{}},{"cell_type":"markdown","source":"## **Adding another Dense layer and Dropout layers**","metadata":{}},{"cell_type":"code","source":"# Adding another dense layer and a couple of dropout layers; structure is modified from one shown on:\n# https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/\nlayers_drop = keras.Sequential()\nlayers_drop.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(200,200,3)))\nlayers_drop.add(layers.MaxPooling2D(2,2))\nlayers_drop.add(layers.Conv2D(64, (3,3), activation='relu'))\nlayers_drop.add(layers.MaxPooling2D(2,2))\n\nlayers_drop.add(layers.Flatten())\nlayers_drop.add(layers.Dense(128, activation='relu'))\nlayers_drop.add(layers.Dropout(0.3))\nlayers_drop.add(layers.Dense(64, activation='relu'))\nlayers_drop.add(layers.Dropout(0.3))\nlayers_drop.add(layers.Dense(1, activation='sigmoid'))\n\nlayers_drop.compile(loss='binary_crossentropy',\n                optimizer='adam',\n                metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# has been rerun\n# Fitting the model\nlayers_drop_results = layers_drop.fit_generator(train_generator,\n                                         steps_per_epoch=2699 // 20+1,# number of samples / batch size\n                                         epochs=10,\n                                         validation_data=test_generator,\n                                        validation_steps= 394 // 20+1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Look at how accuracy and loss change across the epochs, for training and testing data\nvisualize_training_results(layers_drop_results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Analysis of Model**\n\nIn this iteration, training accuracy ends up at 92%, and testing ends up at 70%, so the model is still overfitting. As for loss, training loss is 21% and testing loss is 57%, which is not drastically different from the last model. Testing recall is 65% and testing precision is 91%. In the next model iteration I am going to account for the class imabalance, and the added layer and dropout layers might perform better in this iteration.","metadata":{}},{"cell_type":"markdown","source":"## **Accounting for class imbalance**","metadata":{}},{"cell_type":"code","source":"# Accounting for class imbalance; structure is modified from one shown on:\n# https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/\n\nclass_ld = keras.Sequential()\nclass_ld.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(200,200,3)))\nclass_ld.add(layers.MaxPooling2D(2,2))\nclass_ld.add(layers.Conv2D(64, (3,3), activation='relu'))\nclass_ld.add(layers.MaxPooling2D(2,2))\n\nclass_ld.add(layers.Flatten())\nclass_ld.add(layers.Dense(128, activation='relu'))\nclass_ld.add(layers.Dropout(0.3))\nclass_ld.add(layers.Dense(64, activation='relu'))\nclass_ld.add(layers.Dropout(0.3))\nclass_ld.add(layers.Dense(1, activation='sigmoid'))\n\nclass_ld.compile(loss='binary_crossentropy',\n                optimizer='adam',\n                metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\nweights = {0: 6.255, # NO TUMOR\n          1:1} # TUMOR \n# there are 6.255 times as many images of MRIs with tumors than without","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# has been rerun\n# Fitting the model\nclass_ld_results = class_ld.fit_generator(train_generator,\n                                          class_weight=weights,\n                                         steps_per_epoch=2699//20+1,# number of samples / batch size +1, so that no batch is left out\n                                         epochs=10,\n                                         validation_data=test_generator,\n                                         validation_steps=394//20+1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Look at how accuracy and loss change across the epochs, for training and testing data\nvisualize_training_results(class_ld_results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Analysis of Model**\n\nIn this model iteration, training accuracy was about 87% and testing accuracy is about 49%, so the model is still overfitting, more so than in the last model. Loss for training is at around 58% and testing loss is around 133%. Testing recall is 32% and testing precision is 95%; it is clear that accounting for class imbalance in this way has made the model worse. Further research lead me to a [great article](https://www.analyticsvidhya.com/blog/2020/10/improve-class-imbalance-class-weights/), which discusses how class weights should be calculated. It mentions how adding too much weight to the minority class can sometimes cause the model to show a bias towards it, getting more images in the majority class wrong and making the model less robust. The calculation for what weight to put on the minority class is: total # samples / (2 * number of samples in the minority class). This calculation applied to the training data samples is as follows: 2870/(2*395), which comes out to be about 3.63. So, the class weight for the minority class, in this case \"no_tumor\" MRIs, should be close to 3.63. In the next model, I will see if a class weight of three applied to the no tumor images will be beneficial.","metadata":{}},{"cell_type":"markdown","source":"## **Adjusting Class Weights of Previous Model**","metadata":{}},{"cell_type":"code","source":"# Accounting for class imbalance; structure is modified from one shown on:\n# https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/\n\nadjclass_ld = keras.Sequential()\nadjclass_ld.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(200,200,3)))\nadjclass_ld.add(layers.MaxPooling2D(2,2))\nadjclass_ld.add(layers.Conv2D(64, (3,3), activation='relu'))\nadjclass_ld.add(layers.MaxPooling2D(2,2))\n\nadjclass_ld.add(layers.Flatten())\nadjclass_ld.add(layers.Dense(128, activation='relu'))\nadjclass_ld.add(layers.Dropout(0.3))\nadjclass_ld.add(layers.Dense(64, activation='relu'))\nadjclass_ld.add(layers.Dropout(0.3))\nadjclass_ld.add(layers.Dense(1, activation='sigmoid'))\n\nadjclass_ld.compile(loss='binary_crossentropy',\n                optimizer='adam',\n                metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\n# Incorporating adjusted class weights; compiling and fitting the model\nweights3 = {0: 3, # NO TUMOR\n          1:1.} # TUMOR \n# there are 6.255 times as many images of MRIs with tumors than without, but acording to the class weight calculation, I should \n# use a number close to 3.63 for the minority class, in this case the no tumor, or \"zero\" class.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# has been rerun\n# Fitting the model\nadjclass_ld_results = adjclass_ld.fit_generator(train_generator,\n                                          class_weight=weights3,\n                                         steps_per_epoch=2699//20+1,# number of samples / batch size\n                                         epochs=10,\n                                         validation_data=test_generator,\n                                         validation_steps=394//20+1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_training_results(adjclass_ld_results)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Analysis of model**\n\nTraining accuracy is 91% and testing accuracy is 60%, with a training loss of 35% and a testing loss of 80%. Testing recall is 49% and testing precision is 93%. Although this model still requires much fine tuning, it is clear that adjusting the class weights made it perform a bit better, so these weights will be used from now on. In the next iteration, I will try incorporating class weights.","metadata":{}},{"cell_type":"markdown","source":"## **Model with Batch Normalization** ","metadata":{}},{"cell_type":"code","source":"# Adding Batch Normalization; structure is modified from one shown on:\n# https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/\nclass_n = keras.Sequential()\nclass_n.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(200,200,3)))\nclass_n.add(layers.BatchNormalization())\nclass_n.add(layers.MaxPooling2D(2,2))\nclass_n.add(layers.Conv2D(64, (3,3), activation='relu'))\nclass_n.add(layers.BatchNormalization())\nclass_n.add(layers.MaxPooling2D(2,2))\nclass_n.add(layers.Conv2D(128, (3,3), activation='relu'))\nclass_n.add(layers.BatchNormalization())\nclass_n.add(layers.MaxPooling2D(2,2))\n\nclass_n.add(layers.Flatten())\nclass_n.add(layers.Dense(128, activation='relu'))\nclass_n.add(layers.Dropout(0.3))\nclass_n.add(layers.Dense(1, activation='sigmoid'))\n\nclass_n.compile(loss='binary_crossentropy',\n                optimizer='adam',\n                metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\nweights3 = {0: 3, # NO TUMOR\n          1:1} # TUMOR \n# there are 6.255 times as many images of MRIs with tumors than without","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stop2 = [EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True),\n            ModelCheckpoint(filepath='best_model.h5', monitor='val_loss',\n                           save_best_only=True)]","metadata":{"execution":{"iopub.status.busy":"2021-12-02T05:04:40.013997Z","iopub.execute_input":"2021-12-02T05:04:40.014289Z","iopub.status.idle":"2021-12-02T05:04:40.018642Z","shell.execute_reply.started":"2021-12-02T05:04:40.014255Z","shell.execute_reply":"2021-12-02T05:04:40.01793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_n_results = class_n.fit_generator(train_generator,\n                                          class_weight=weights3,\n                                         steps_per_epoch=2699//20+1,# number of samples / batch size\n                                         epochs=20,\n                                         callbacks=early_stop2,\n                                         validation_data=test_generator,\n                                         validation_steps=394//20+1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Look at how accuracy and loss change across the epochs, for training and testing data\nvisualize_training_results(class_n_results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Analysis of Model**\n\n In the epoch with the lowest testing loss, training accuracy is 91% and testing accuracy is 66%, with a training loss of 30% and a testing loss of 56%. Testing recall is 57% and testing precision is 96%. Loss has been decreased in this model iteration compared to the last.\n \n Because batch normalization makes the network more stable, it is possible to use larger learning rates, which could potentially help the model reach optimal accuracy and minimal loss more quickly, so that is what I will try next.","metadata":{}},{"cell_type":"markdown","source":"## **Using a Bigger Learning Rate since I am using Batch Normalization**","metadata":{}},{"cell_type":"code","source":"# Establishing an instance of Adam with a bigger learning rate\nadam_mlr = keras.optimizers.Adam(epsilon=0.01)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using Batch Normalization; structure is modified from one shown on:\n# https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/\nclass_na = keras.Sequential()\nclass_na.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(200,200,3)))\nclass_na.add(layers.BatchNormalization())\nclass_na.add(layers.MaxPooling2D(2,2))\nclass_na.add(layers.Conv2D(64, (3,3), activation='relu'))\nclass_na.add(layers.BatchNormalization())\nclass_na.add(layers.MaxPooling2D(2,2))\nclass_na.add(layers.Conv2D(128, (3,3), activation='relu'))\nclass_na.add(layers.BatchNormalization())\nclass_na.add(layers.MaxPooling2D(2,2))\n\nclass_na.add(layers.Flatten())\nclass_na.add(layers.Dense(128, activation='relu'))\nclass_na.add(layers.Dropout(0.3))\nclass_na.add(layers.Dense(1, activation='sigmoid'))\n\nclass_na.compile(loss='binary_crossentropy',\n                optimizer=adam_mlr,\n                metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\nweights3 = {0: 3, # NO TUMOR\n          1:1} # TUMOR \n# there are 6.255 times as many images of MRIs with tumors than without","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using a bigger learning rate; fitting the model\nclass_na_results = class_na.fit_generator(train_generator,\n                                          class_weight=weights3,\n                                         steps_per_epoch=2699//20+1,# number of samples / batch size\n                                         epochs=20,\n                                         callbacks=early_stop2,\n                                         validation_data=test_generator,\n                                         validation_steps=394//20+1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Look at how accuracy and loss change across the epochs, for training and testing data\nvisualize_training_results(class_na_results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Analysis of Model**\n\nIn the epoch with the lowest testing loss, training accuracy is 91% while testing accuracy is 75%. Training loss is 27%, while testing loss is 43%. \nTesting recall is 89% and testing precision is 90%. Testing accuracy is a little greater and testing loss is a bit lower than in the last iteration, so it looks like using a bigger learning rate in combination with Batch Normalization was a good idea. ","metadata":{}},{"cell_type":"markdown","source":"## **Model with Batch Normalization and more Dropout Layers**","metadata":{}},{"cell_type":"code","source":"# Accounting for the class imbalance; structure is modified from one shown on:\n# https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/\nclass_nd = keras.Sequential()\n\nclass_nd.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(200,200,3)))\nclass_nd.add(layers.BatchNormalization())\nclass_nd.add(layers.MaxPooling2D(2,2))\nclass_nd.add(layers.Dropout(0.25))\n\nclass_nd.add(layers.Conv2D(64, (3,3), activation='relu'))\nclass_nd.add(layers.BatchNormalization())\nclass_nd.add(layers.MaxPooling2D(2,2))\nclass_nd.add(layers.Dropout(0.25))\n\nclass_nd.add(layers.Conv2D(128, (3,3), activation='relu'))\nclass_nd.add(layers.BatchNormalization())\nclass_nd.add(layers.MaxPooling2D(2,2))\nclass_nd.add(layers.Dropout(0.25))\n\nclass_nd.add(layers.Flatten())\nclass_nd.add(layers.Dense(128, activation='relu'))\nclass_nd.add(layers.Dropout(0.4))\nclass_nd.add(layers.Dense(1, activation='sigmoid'))\n\nclass_nd.compile(loss='binary_crossentropy',\n                optimizer='adam',\n                metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\nweight3 = {0: 3, # NO TUMOR\n          1:1} # TUMOR \n# there are 6.255 times as many images of MRIs with tumors than without","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fitting the model\nclass_nd_results = class_nd.fit_generator(train_generator,\n                                          class_weight=weights3,\n                                         steps_per_epoch=2699//20+1,# number of samples / batch size\n                                         epochs=20,\n                                          callbacks=early_stop2,\n                                         validation_data=test_generator,\n                                         validation_steps=394//20+1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Look at how accuracy and loss change across the epochs, for training and testing data\nvisualize_training_results(class_nd_results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Analysis of Model**\n\nTraining accuracy of the epoch with the lowest loss is 86% while testing accuracy is 74%. Training loss is 41% while testing loss is 68%. Testing recall is 100% and testing precision is 74%. Training and Testing loss have both increased, so the dropout layers added after each max pooling step may not be particularly beneficial to the model. Next, I will try using pretrained neural networks.","metadata":{}},{"cell_type":"markdown","source":"## **Using the Pre-Trained VGG-19 Weights (this is my FSM)**","metadata":{"execution":{"iopub.status.busy":"2021-11-24T20:11:41.921375Z","iopub.execute_input":"2021-11-24T20:11:41.921882Z","iopub.status.idle":"2021-11-24T20:11:41.943521Z","shell.execute_reply.started":"2021-11-24T20:11:41.92184Z","shell.execute_reply":"2021-11-24T20:11:41.942711Z"}}},{"cell_type":"code","source":"from keras.applications.vgg19 import VGG19\ncnn_vgg = VGG19(weights='imagenet',\n               include_top=False,\n               input_shape=(200,200,3))","metadata":{"execution":{"iopub.status.busy":"2021-12-02T05:04:45.291721Z","iopub.execute_input":"2021-12-02T05:04:45.292278Z","iopub.status.idle":"2021-12-02T05:04:46.673144Z","shell.execute_reply.started":"2021-12-02T05:04:45.292236Z","shell.execute_reply":"2021-12-02T05:04:46.672361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn_vgg.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build first model using pretrained VGG 19 as first layer, and then some dense layers on top\npretrained = keras.Sequential()\npretrained.add(cnn_vgg)\npretrained.add(layers.Flatten())\npretrained.add(layers.Dense(128, activation='relu'))\npretrained.add(layers.Dense(1, activation='sigmoid'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Freeze_Pretrained_Base(pretrain, network):\n    pretrain.trainable = False\n    for layer in network.layers:\n        print(layer.name, layer.trainable)\n    print(len(network.trainable_weights))","metadata":{"execution":{"iopub.status.busy":"2021-12-02T05:04:49.221627Z","iopub.execute_input":"2021-12-02T05:04:49.221904Z","iopub.status.idle":"2021-12-02T05:04:49.227118Z","shell.execute_reply.started":"2021-12-02T05:04:49.221874Z","shell.execute_reply":"2021-12-02T05:04:49.226364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Freeze_Pretrained_Base(cnn_vgg, pretrained)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\npretrained.compile(loss='binary_crossentropy',\n                optimizer='adam',\n                metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\n\npretrained_results = pretrained.fit_generator(train_generator,\n                                         steps_per_epoch=2699//20+1,# number of samples / batch size\n                                         epochs=20,\n                                        callbacks= early_stop2,\n                                         validation_data=test_generator,\n                                             validation_steps=394//20+1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Look at how accuracy and loss change across the epochs, for training and testing data\nvisualize_training_results(pretrained_results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Analysis of Model**\n\nThe epoch with the lowest testing loss has a training accuracy of 95% and a testing accuracy of 82%, with a training loss of 14% and a testing loss of 45%. Testing recall is 87%, and testing precision is 88%. Testing loss is lower and testing accuracy is higher than in the previous model, so using pretrianed models looks promising. In the next iteration, I will try fine tuning this network by adding dropout layers and unfreezing some of the outer layers of the pretrained network so they can learn from the images in this dataset.","metadata":{}},{"cell_type":"markdown","source":"## **Unfreezing Layers in the Pretrained VGG-19 Network**","metadata":{}},{"cell_type":"code","source":"# Build first model using pretrained VGG 19 as first layer, and then some dense layers on top\nb5c1c2 = keras.Sequential()\nb5c1c2.add(cnn_vgg)\nb5c1c2.add(layers.Flatten())\nb5c1c2.add(layers.Dense(128, activation='relu'))\nb5c1c2.add(layers.Dense(1, activation='sigmoid'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List for unfreezing of layers function\nunfreeze = ['block5_conv1', 'block5_conv2']","metadata":{"execution":{"iopub.status.busy":"2021-12-02T05:04:53.415689Z","iopub.execute_input":"2021-12-02T05:04:53.416245Z","iopub.status.idle":"2021-12-02T05:04:53.420166Z","shell.execute_reply.started":"2021-12-02T05:04:53.416204Z","shell.execute_reply":"2021-12-02T05:04:53.419473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Unfreezing some of outer layers of VGG19 pretrained network\nUnfreeze_Layers(cnn_vgg, unfreeze)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Re-freezing everything except for the last layer of the pretrained CNN\n# Code structure from https://github.com/learn-co-curriculum/dsc-using-pretrained-networks-codealong\ndef Unfreeze_Layers(pretrain, layer_list):\n    pretrain.trainable = True\n    for layer in  pretrain.layers:\n        if layer.name in layer_list:\n            layer.trainable = True\n        else:\n            layer.trainable = False\n        \n    for layer in pretrain.layers:\n        print(layer.name, layer.trainable)\n    print(len(pretrain.trainable_weights))","metadata":{"execution":{"iopub.status.busy":"2021-12-02T05:04:55.498309Z","iopub.execute_input":"2021-12-02T05:04:55.499124Z","iopub.status.idle":"2021-12-02T05:04:55.506184Z","shell.execute_reply.started":"2021-12-02T05:04:55.499079Z","shell.execute_reply":"2021-12-02T05:04:55.505361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compiling and Fitting the model\nb5c1c2.compile(loss='binary_crossentropy',\n                optimizer='adam',\n                metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\n\nb5c1c2_results = b5c1c2.fit_generator(train_generator,\n                                         steps_per_epoch=2699//20+1,# number of samples / batch size\n                                         epochs=20,\n                                        callbacks= early_stop2,\n                                         validation_data=test_generator,\n                                         validation_steps=394//20+1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compiling and Fitting the model\nb5c1c2.compile(loss='binary_crossentropy',\n                optimizer='adam',\n                metrics=['acc', 'Recall', 'Precision', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives'])\n\nb5c1c2_results = b5c1c2.fit_generator(train_generator,\n                                         steps_per_epoch=2699//20+1,# number of samples / batch size\n                                         epochs=20,\n                                        callbacks= early_stop2,\n                                         validation_data=test_generator,\n                                         validation_steps=394//20+1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Analysis of Model**\n\nIn the epoch with the lowest epoch, training accuracy is 99% and testing accuracy is 91%, with a training loss of 5% and a testing loss of 27%. Testing recall is 89% and testing precision is 98%. Testing accuracy and loss have increased considerably since the last iteration! Unfreezing some of the outer layers of the pretrained network seems to have helped the model better learn patterns in the MRI images.","metadata":{}}]}